\documentclass[a4paper,10pt,titlepage]{article}
\usepackage[utf8]{inputenc}

\usepackage{pattern}
\usepackage{todo}

% Nice fonts
\usepackage{palatino}
% Needed for Listings package with Eiffel.
\usepackage{xcolor}
% Source code listings.
\usepackage{listings}
% Appendix with extra title.
\usepackage [toc, page] {appendix}
% To include PNG files.
\usepackage{graphicx}
% Nice looking captions.
\usepackage[font={footnotesize,sl}, labelfont=bf] {caption}

% Clickable links. Has to be the last package:
\usepackage [hidelinks] {hyperref}


\lstset{language=OOSC2Eiffel,basicstyle=\ttfamily\small}
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\setlength{\headheight}{28pt}
\lstset{escapechar=\$}

% \newcommand{\dir} [1] [] {\emph{#1}} 
\newcommand{\dir}{\emph}
\newcommand{\todoref}{\todo{ref}}

% Title Page
\title{Concurrency Patterns in SCOOP}
\author{Roman Schmocker \\ \\ Supervised by Alexey Kolesnichenko \\ and Prof. Dr. Bertrand Meyer}


\begin{document}

\pagenumbering{roman}

\maketitle
\thispagestyle{empty}
\newpage



\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{2}
The wide distribution of multi-core processors increasingly forces programmers to deal with concurrency.
Parallel programming is not easy, but there are many well-known patterns at hand to help developers.
% Parallel programming is not easy, but it can be simplified with the use of well-known patterns.

SCOOP, an extension to the Eiffel programming language, provides an alternative approach to concurrent programming compared to the threading mo\-del used in many other languages.
There is little experience in implementing and using concurrency patterns in SCOOP however.

% In SCOOP, an extension to the Eiffel programming language, there's neither a library nor experience in using concurrency patterns.
We have investigated which patterns are used in practice and compiled an exhaustive list of pattern descriptions.
From this list we selected several popular concurrency patterns and implemented them as a reusable Eiffel library.
A small performance comparison shows that the new library is more robust and faster for large data sets than a raw SCOOP solution.
% Some of these patterns were then implemented as a reusable library for Eiffel.
We also describe some of the challenges when programming in SCOOP for the first time and provide solutions.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{3}
I would like to thank Alexey Kolesnichenko very much for his great support and helpful advice throughout my thesis.
Many thanks go to Prof. Bertrand Meyer for his insightful comments and for giving me the opportunity to work on this project.
I would also like to thank Julian Tschannen, Mischael Schill, Scott West, Sebastian Nanz and others at the Chair of Software Engineering for their useful input.

\emph{Roman Schmocker}
\end{abstract}


\setcounter{page}{4}
\tableofcontents

\newpage
\pagenumbering{arabic}

\section{Introduction}
\label{sec:introduction}


Due to the advent of multicore processors, concurrent programming has become an important part in software engineering.
Dealing with parallelism isn't easy however.
There are many pitfalls, such as race conditions and deadlocks.

In practice programmers have learned to avoid tricky concurrency problems with the use of some well-known patterns.
These patterns are often shipped as part of the standard library of the language, such that users rarely have to implement them.

The Eiffel programming language \cite{web:ecma-eiffel}\cite{book:touchofclass} has an extension called SCOOP \cite{Nienaltowski07}\cite{web:scoop},
which stands for Simple Concurrent Object-Oriented Programming.
SCOOP simplifies concurrent programming a lot and eliminates one source of errors completely, namely race conditions \cite{Nienaltowski07}.
However, there is little experience on how to implement popular concurrency patterns, like a worker pool, in SCOOP.

This thesis tries to fill this gap by providing a library of reusable concurrency patterns as well as methodical advice on programming in SCOOP.
The main contributions are:
\begin{itemize}
 \item A broad survey of known concurrency patterns.
 \item The identification of common SCOOP challenges and advice on how to solve them.
 \item A new library which provides implementations for some selected concurrency patterns.
 This selection was mainly based on input from the Software Engineering group at ETH ZÃ¼rich and the study of Java \cite{web:java-concurrency} and C\# \cite{web:ms-tpl} concurrency libraries.
\end{itemize}

\subsection{Overview}

Section \ref{sec:pattern_overview} introduces a list of concurrency patterns which we found and categorized by studying literature and the standard libraries.
A brief introduction of the SCOOP model is given in Section \ref{sec:scoop-model}.
Section \ref{sec:scoop-challenges} describes some challenges when programming in SCOOP and how to solve them.
The latter two sections may be interesting for programmers having experience in threaded programming and who wish to learn SCOOP.

The focus of Section \ref{sec:library} is on the goals and concepts of the concurrency patterns library.
It also provides an overview over the available modules and describes which patterns are implemented by which modules.

A detailed explanation over the individual modules is given by Section \ref{sec:modules}.
Finally, Section \ref{sec:evaluation} provides a small performance evaluation of the library.

\section {Pattern overview}
\label{sec:pattern_overview}
% Overview of all patterns, maybe tabular

\input{pattern_list}

\section {The SCOOP model}
\label {sec:scoop-model}
% Introduction, differences to Java etc... (short)

SCOOP is an extension to the Eiffel programming language that aims to make concurrent programming easier.
The basic idea is that every object can be accessed by exactly one computational unit only.
This unit is called processor or handler of an object.

The keyword \lstinline!separate! is used to indicate that an object may be handled by a different processor than the handler for \lstinline!Current!.
Calls to a separate object (``separate calls'') then correspond to sending a message to the foreign processor.
There are two types of separate calls: synchronous and asynchronous.
If the called feature returns a result the call is synchronous, which means that the current processor has to wait for the foreign processor to finish its task.
An asynchonous call happens when the feature is a command, i.e. not returning any result.
In that case both processors can proceed concurrently.

A separate call is only allowed if its target is ``controlled''.
Controlling an object means that the user has exclusive access to that object.
In that sense controlling corresponds a bit to locking in other languages.
In order to control an object it has to appear as a formal argument in the enclosing routine.

SCOOP guarantees that all messages sent by the current processor are handled in the correct order by the foreign processor.
The exclusive access and order guarantee ensure that a controlled separate object behaves just like an object in a sequential program.
This is the reason why the SCOOP model is so simple: 
It allows reasoning about a feature body without the need to consider all possible interleavings of two parallel executions.

A new processor is created by calling a creation instruction on a variable which is declared as separate.
The new object is then handled by the new processor.

Preconditions in SCOOP have a special role.
In a concurrent setting there's often the problem that a correctness condition may change due to unfortunate interleaving, 
e.g. between checking that a buffer is not empty and then removing an item, the buffer actually becomes empty due to interference from another thread.
Therefore SCOOP turns preconditions into wait conditions if they reference a separate object.

There are many advantages to the SCOOP model, such as easier reasoning and absence of data races, but it also has some shortcomings.
It is for example often necessary to write lots of little helper functions that just take a separate object and perform a single call on it,
because SCOOP enforces taht every target of a separate call needs to be controlled.

SCOOP also has performance problems because it transforms every separate call into a message to another processor.
This is rather expensve, especially for small functions like array access.

Furthermore, a processor is currently implemented as an operating system thread and creating them is a costly operation that involves context switches.
The SCOOP model however encourages the creation of many processors which is not ideal for performance reasons.

\section{Challenges in SCOOP}
\label{sec:scoop-challenges}
% This section describes recurring challenges in SCOOP and how to solve them...

\subsection{Object migration}
\label{sec:object-migration}

Passing data from one processor to another is often necessary when programming in SCOOP.
The most obvious example is the Producer / Consumer pattern \patternref{P/C}, but it also applies to other situations like providing some arguments to an asynchronous command.

There are two categories of objects which can be passed as arguments: expanded and reference types.
Passing expanded objects, which also includes basic types such as \lstinline!INTEGER!, is not a problem in SCOOP due to their copy-semantics property.
However, passing a reference object from one processor to another is a bit more tricky
because bad things such as starvation or unintentional lock passing may happen if done wrong.

There are essentially three ways to safely move reference objects from a sender to a receiver processor.
The first and easiest solution is to create the data on its own, separate processor: 
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Migrate objects on a separate processor.}]
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: separate ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This approach is conceptually easy but not very efficient, especially when the argument object is small.
We'll call this solution the Data Processor approach.

Another solution is to create the object on the same handler as the sender object:
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Migrate objects with lock passing.}]
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This solution (the Lock Passing approach) looks almost like the first one.
The only change is a missing separate keyword.
The semantics however are radically different:

\begin{itemize}
 \item The feature \lstinline!do_something! is executed synchronously due to the lock passing mechanism \cite[p. 152]{Nienaltowski07}\cite{web:scoop}.
 This means that the sender class needs to wait for it to finish.
 \item \lstinline!RECEIVER! can't lock the argument object any more after \lstinline!do_something! finishes.
 In particular this means that the receiver class should not store the argument in one of its attributes, because any attempt to access it later will likely result in starvation.
 The reason for this is that the handler of \lstinline!SENDER! will continue its execution, and as long as there's still work to do no other processor can access objects on it .
 \item Compared to the first approach no new processor is created.
\end{itemize}

The last method makes use of a special SCOOP mechanism called \lstinline!import!:
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Migrate objects with import.}]
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.receive_args (args)
      a_receiver.do_something
    end
end

class RECEIVER feature
  
  received: ANY
  
  receive_args (args: separate ANY)
      -- Receive some arguments
    do
      received := import (args)
    end

  do_something
      -- Perform some operation.
    do
      print (received)
    end
end
\end{lstlisting}
The \lstinline!import! feature copies its argument along with all non-separate references to the local processor.
It is somewhat similar to \lstinline!{ANY}.deep_copy!, except that it doesn't follow separate references.

The import solution has several advantages.
There is no need for a new processor, and the receiver can keep the data and do the operation asynchronously.
The drawback is that the data needs to be copied.
However, for small data items this is actually faster than creating a new thread.

Note that \lstinline!receive_args! is executed synchronously just as in the Lock Passing approach.
To execute \lstinline!do_something! asynchronously it has therefore been divided into an execution and argument receiving part.

The feature \lstinline!import! was first described in \cite[p. 106]{Nienaltowski07}, but unfortunately it is not implemented in current SCOOP.
It is possible however to implement it manually with some user support.

\subsection{Processor communication}
\label{sec:processor-communication}
% Problem that two concurrent, active processors can't communicate. Example downloader task.
% Solution: a third, passive processor with a shared data structure.

It is often the case that two threads need to communicate with each other.
An example would be a user interface with a background download task.
The user interface needs to be able to cancel the download, and the download task has to inform the GUI when it is finished.

In SCOOP this is not easily done.
Both processors are performing a long-running execution, which doesn't allow other processors to do separate calls on them.
Specifically, the GUI processor is in an infinite loop to receive input and repaint the window, whereas the download task is busy receiving chunks of data.
Cancellation will not work in this case because the user interface processor will have to wait for the download processor to finish until it can actually access the download task to call \lstinline!cancel!, 
which kind of defeats the purpose of the cancellation button.
Worse yet, the user interface will freeze until the GUI processor finally gets the lock.

The solution to this kind of problem is to introduce a third processor which is ``passive'', meaning that it doesn't have a task to perform and only waits for incoming requests.
This new processor is known to the other two, ``active'' processors and handles an object which can be used for communication.
In our example the ``passive'' processor has an object with an \lstinline!is_cancelled! and \lstinline!is_finished! boolean flag.
The ``active'' processors then regularly need to check the status of these flags.

The solution to the task cancellation problem comes from a previous paper by the Software Engineering group \cite{paper:task-cancellation}.

\subsection{Processor termination}
\label{sec:processor-termination}
% Problem: how to stop consumer waiting on empty buffer.
% Solution: Query is_stopped in shared buffer.

When an application terminates it is necessary to stop any running thread.
Sometimes this can be done with processor communication as seen in Section \ref{sec:processor-communication}.
A problem arises however when a processor is stuck in a wait condition.

One example of this could be a producer / consumer situation where a consumer is waiting for the buffer to become non-empty.
If the producers have terminated already, the consumer never gets the chance to break out of its wait condition and therefore cannot terminate successfully.

The solution is to add a query \lstinline!is_stop_requested! in the shared buffer and to adapt the wait condition to include the stop request:

\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Breaking out of a wait condition.}]
class
  CONSUMER

feature -- Status report

  buffer: separate BUFFER
  
  last_item: INTEGER
  
  is_stopped: BOOLEAN
  
feature -- Basic operations
  
   start
      -- Start the main loop
    do
      from
        fetch (buffer)
      until 
        is_stopped
      loop
          -- Do something, e.g.
        print (last_item)

        fetch (buffer)
      end
    end
  
feature -- Implementation

  fetch (buf: separate BUFFER)
      -- Get the next item from `buf'.
    require
      not buf.is_empty or buf.is_stop_requested
    do
      if buf.is_stop_requested then
        is_stopped := True
      else
        last_item := buf.item
        buf.remove
      end
    end
end
\end{lstlisting}

This allows a consumer to leave the wait condition even when the buffer is empty.
The drawback of this approach is that it clutters the application code with some additional if-else constructs, 
but it is often possible to hide them in a \lstinline!fetch! function, as shown in our example.

\section {Library}
\label{sec:library}

% The library is designed as a set of modules which simplify concurrent programming in SCOOP.
% Section ~\ref{sec:tutorial} explains how to use the library for commonly used patterns,
% while Section ~\ref{sec:modules} concentrates on the design of the library itself.
 
\subsection{Goals}
\label{sec:goals}

The goal of the library is to provide a set of classes that simplify programming in SCOOP. 
Specifically, we want to provide implementations for common concurrency patterns like the worker pool.
The result should be a new Eiffel library similar to the standard concurrency libraries in Java \cite{web:java-concurrency} or C\# \cite{web:ms-tpl}.

The library was developed with the following design goals:

\begin{description}
 \item [Safety]\label{item:safety} Avoid common SCOOP pitfalls like deadlocks, starvation of a processor or unintentional lock passing.
 \item [Convenience]\label{item:convenience} Shield the user from having to write many little ``wrappers'', i.e. features that just lock an object for a single separate call.
 \item [Performance]\label{item:performance} Reduce the overhead of thread creation, especially for code that deals with a lot of small separate objects.
\end{description}

\subsection{Concepts}

This section describes two core concepts of the library: Import and Separate Proxies.
The import concept deals with the problem of how to pass data from one processor to another.
It is useful to achieve the performance and to some extent the safety design goal in Section \ref{sec:goals}.

The Separate Proxy \patternref{SP} is a pattern to hide separate references behind a proxy object.
It provides a solution to the convenience design goal.

\subsubsection{Import}
\label{sec:concepts:import}
% Describe how to use it, i.e. generic parameter and importer objects, and the fact that you can select between import and no import.

The import concept is a central part of the library.
It was developed to let users choose between two object passing strategies, namely the Data Processor and the Import approach (see Section \ref{sec:object-migration}).

The main class is \lstinline!CP_IMPORT_STRATEGY!, which has the simple interface:

\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={The deferred class CP\_IMPORT\_STRATEGY.}]
deferred class interface
  CP_IMPORT_STRATEGY [G]

feature -- Status report

  is_importable (object: separate G): BOOLEAN
      -- Is `object' importable?

feature -- Duplication

  import (object: separate G): separate G
      -- Import `object'.
    require
      importable: is_importable (object)

end
\end{lstlisting}

%\lstinputlisting [firstline=7] {../../library/import/cp_import_strategy.e}

The class has two descendants.
\lstinline!CP_NO_IMPORTER[G]! can be used for the Data Processor strategy. 
It just perform a reference copy of the object.
The class \lstinline!CP_IMPORTER [G]! on the other hand narrows the return type of \lstinline!import! to a non-separate \lstinline!G!, meaning that it actually performs an import.

As there's no general-purpose import feature available in SCOOP at the moment, users have to implement their own import features for every class that needs this facility.
Descendants of \lstinline!CP_IMPORTER! simplify this task and provide predefined implementations for some standard classes such as \lstinline!STRING!.
Those mechanisms are described in detail in Section \ref{sec:modules:import}.

Components that want to make use of the import mechanism need to have an instance of \lstinline!CP_IMPORT_STRATEGY! on the same processor.
There are several ways how this object can be supplied to a library component.
The most obvious solution - passing it as an argument in a constructor - has a big drawback in the SCOOP world:
It is impossible to instantiate the component on a separate processor without writing an extra factory class.

A better solution is to exploit the constrained genericity mechanism in Eiffel.
A component that needs to import objects has to declare an additional generic argument for the import strategy.
A user can then decide on the precise semantics of the import strategy by just declaring the right type.

The constraint placed on the generic argument is that it needs to be a descendant of \lstinline!CP_IMPORT_STRATEGY! and that it needs to declare \lstinline!default_create! as a creation procedure.
The latter is not a big restriction in practice, as there are usually no attributes in an importer anyway.

A typical class header of a component using the import concept looks like this:
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={An example component with import.}]
class BUFFER [G, IMPORTER -> 
          CP_IMPORT_STRATEGY [G] create default_create end]

feature -- Access

  item: like {IMPORTER}.import

end
\end{lstlisting}

This small code sample shows another neat little feature of Eiffel.
The \lstinline!like! statement fixes the type based on the chosen import strategy, i.e. \lstinline!separate G! for \lstinline!CP_NO_IMPORTER! and non-separate for descendants of \lstinline!CP_IMPORTER!.
This makes the handling of imported objects a lot easier.

\subsubsection{Separate Proxy}

The Separate Proxy pattern \patternref{SP} simplifies access to a separate reference by providing a processor-local proxy object wich forwards all requests to the actual object.
The main advantage is that clients don't need to write extra ``wrapper'' feature to control the separate object.
It is applied to all classes in the library which are meant to be shared among processors, i.e. which are usually accessed through a separate reference.

The pattern consists of three classes:
\begin{description}
 \item [ProtÃ©gÃ©] The actual business class whose objects are usually separate.
 \item [Helper] A class that provides helper functions to access a separate protÃ©gÃ©.
  The helper class is usually ending on \lstinline!_UTILS!.
 \item [Proxy] A proxy class with a similar interface as the protÃ©gÃ© class, usually ending on \lstinline!_PROXY!.
    The proxy forwards every call to its protÃ©gÃ©, using the helper class.
\end{description}

It is possible to add a fourth, deferred class that just defines a common interface for the protÃ©gÃ© and the proxy.
There's an inconsistency however: 
All preconditions in the protÃ©gÃ© class that reference \lstinline!Current! (explicitly or implicitly) need to be weakened (i.e. \lstinline!require else True!) in the proxy, and turned into wait conditions in the helper class.
Furthermore, not all features in the business class may be necessary in the proxy, and the proxy itself may add some more features such as compound actions.

\begin{figure}[h]
\label{fig:separate-proxy}
\includegraphics[width=\textwidth]{separate_proxy.png}
\caption{The class relations in the Separate Proxy pattern.}
\end{figure}

Unfortunately this pattern cannot be turned into a reusable module, because it is highly dependent on the precise interface of the protÃ©gÃ© class.
There is some support in the library however: 
\lstinline!CP_PROXY! defines a creation procedure and the attributes \lstinline!subject! for a separate protÃ©gÃ© object and \lstinline!utils! for a helper object.

Appendix \ref{sec:howto-separate-proxy} provides a general recipe on how to implement a separate proxy for an arbitrary protÃ©gÃ© class.

\subsection {Module overview}
\label{sec:module-overview}
% Describe available modules, which patterns they're implementing, and which modules they depend upon.
The library consists of several modules which implement some of the patterns described in the overview (Section \ref{sec:pattern_overview}).
The source code of the library is available on GitHub \cite{web:repository}.
All file locations in the following section are relative to the root directory of the repository.

One of the most basic modules is the import module in \dir{library/import}.
It implements the Import \patternref{IMP} pattern  and is at the same time one of the core concepts of the library.

The queue module in \dir{library/queue} implements the Prdocuer / Consumer \patternref{P/C} pattern.
It depends on the import module.

The process module in \dir{library/process} provides skeleton classes for objects with a main loop.
It provides implementations for the Active Object \patternref{AO}, Asynchronous Self-Call \patternref{ASC} and Timer: Periodic \patternref{TP} patterns.

The worker pool module in \dir{library/worker\_pool} implements the Worker Pool \patternref{WP} pattern.
It uses the import, queue and process module.

The directory \dir{library/promise} contains the promise module.
This module provides classes to monitor and interact with an asynchronous operation.

The executor module resides in \dir{library/executor} and provides an implementation to the Executor Framework \patternref{EF}, 
a specialized worker pool and part of the implementation to the Future pattern \patternref{FUT}.

The implementation of the future pattern is highly intertwined with other parts of the library.
It makes use of the promise, executor and worker pool modules and introduces only two classes on its own:
\lstinline!CP_COMPUTATION! and \lstinline!CP_FUTURE_EXECUTOR_PROXY!.

The class \lstinline!CP_DELAYED_TASK! in \dir{libary/util} implements the Timer: Invoke Later pattern \patternref{TIL}.
The same directory also contains the class \lstinline!CP_EVENT!, which implements the Publish / Subscribe pattern \patternref{P/S} in SCOOP.

\input{modules}

\section{Evaluation}
\label {sec:evaluation}

To evaluate the library we did a small performance benchmark.
We implemented the Gaussian elimination algorithm in three different ways: sequentially, with SCOOP only, and using the future module (see Section \ref{sec:futures}) from our library.
We chose to use the future module because it indirectly also measures many other parts of the library like the worker pool or import mechanism.

We ran the tests with randomly generated square matrices.
The order of a matrix was always a power of two in the range from 32 to 1024.
Additionally, there was one more column for the result vector in the system of linear equations.
Each test was repeated 5 times.
The test system was a quad-core AMD Phenom II X4 955 processor with 6 GB of RAM.
The results are shown Table \ref{table:perf-results}.

\begin{table} [h]
\centering
\begin{tabular}{|l|l l l|} 
\hline
Matrix Size & Future & Raw SCOOP & Sequential\\
\hline
32 & 0.36 & 0.19 &  \textless 0.01\\
64 & 1.67 & 1.11 & \textless 0.01\\
128 & 8.45 & 9.27 & 0.04\\
256 & 26.45 & 66.56 & 0.33\\
512 & 102.28 & 515.11 & 2.62\\
1000 & - &  3937.2 & - \\
1024 & 424.32 & error & 20.79 \\
\hline
\end{tabular}
\caption{Average time in seconds for different algorithms.}
\label{table:perf-results}
\end{table}

\todo{Maybe add small data plot.}

We can get several observations from the results:

\begin{itemize}
 \item The raw SCOOP solution fails for the biggest matrix.
 This is a known bug \cite{web:scoop-issues}:
 The algorithm uses more than the maximum number of processors.
 The library solution doesn't suffer from this problem because it's using a fixed amount of processors.
 \item Futures are faster than raw SCOOP for large data sets.
 \item For smaller data sets, raw SCOOP beats the library.
 \item Sequential execution is much faster than SCOOP.
\end{itemize}

The last observation is probably the most fundamental.
The SCOOP runtime really needs to be improved in order to make it competitive to threaded systems, or even sequential ones.
Fortunately an improved version \cite{thesis:scottwest} is being developed at the time of writing.
It will probably be integrated into a future EiffelStudio release.

Another improvement which might be useful for the library is the Passive Processor concept \cite{paper:passive-processors}.
If both the worker pool and the promise processor could be declared passive the computation speed up a lot.

\section{Conclusion}
% Explain what is the impact of the selected patterns, and why they were selected.
% Say that import was developed to overcome language limitation
% Also: Future work

%Applications using multiple processor cores are increasingly becoming the norm, despite the difficulties of dealing with parallelism.
Concurrent programming is increasingly becoming the norm despite its difficulties.
The SCOOP model provides a solid foundation to concurrent programming in Eiffel, 
but it is hard to learn for developers due to the paradigm shift and the lack of a concurrency library.

In this thesis we've worked out many methods that simplify concurrent programming in SCOOP.
We did a broad survey of popular concurrency patterns and present our findings in a detailed list.
The list can be used to search for a pattern that fits a particular problem, which may even be useful to programmers of threaded languages.
% We've done a survey of concurrency patterns and described them in an extensive list.
% This list can be used by anyone searching for a particular pattern.

From our survey we selected some patterns which we thought to be especially useful.
The selection was based on the study of other concurrency libraries as well as some input from the Software Engineering research group at ETH.
The seleted patterns were then implemented and are now available as a new Eiffel library.
Besides the pattern implementations, the library also provides some workarounds for current SCOOP limitations, such as the missing import statement.

Performance measurements for the Future pattern showed that the library is actually faster for large data sets and uses less threads than the native SCOOP approach.

Finally, we also described some challenges when programming in SCOOP and how to solve them.
This is especially useful to developers experienced in threaded programming who want to use SCOOP for the first time.

\subsection{Future work}

The library provides several opportunities for future work.

\begin{description}
 \item [More patterns] The library can be extended with additional patterns.
 It may be useful to include Pipeline or Dataflow Network.
 \item [Separate Proxies] The Separate Proxy pattern may be applied to some EiffelBase classes, such as \lstinline!ARRAYED_LIST!, \lstinline!HASH_TABLE! or \lstinline!ROUTINE!.
 \item [Separate Proxy Wizard] Writing a Separate Proxy is tedious. Most of it could be automated with a wizard however.
% \item [EiffelVision support] \todo{maybe?}
% \item [Agent integration] \todo{maybe?}
 \item [Concurrent Datastructures] Sometimes it may be useful to have truly concurrent data structures for performance reasons.
The Array Slicing technique \cite{paper:array-slicing} is an example how arrays with concurrent read access can be implemented in SCOOP.
\end{description}

SCOOP itself also provides a rich offering of possible improvements.
% A important step is to improve SCOOP itself.
% There are various ways to do it, most of them related to performance improvements.

\begin{description}
 \item [Faster runtime] The SCOOP runtime needs to become faster. 
 This is currently being developed \cite{thesis:scottwest}.
 \item [Native Import] A native SCOOP import mechanism is a great tool to deal with a lot of small objects.
 It will also make the library API and implementation simpler, as the manual import workaround can be removed.
 \item [Passive Processors] The passive processors concept \cite{paper:passive-processors} could be integrated into EiffelStudio.
 It can make a big performance improvement to situations where one needs to pass data from one processor to another.
 \item [Separate references] The handling of separate references should become more convenient.
 At the moment programmers are forced to write a lot of small, annoying features to perform separate calls.
 Some syntactic sugar would be really helpful.
\end{description}


\newpage
\begin{flushleft}
{{{
\bibliographystyle {plain}
\bibliography {./references}
}}}
\end{flushleft}

\newpage
\begin{appendices}
% \input{tutorial}
\input{separate_proxy}
\section{SCOOP and EiffelVision}
\input{eiffelvision}
\end{appendices}

\todos

\end{document}          
