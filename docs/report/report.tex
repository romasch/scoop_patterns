\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{pattern}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{todo}
\usepackage{appendix}

% Has to be the last package:
\usepackage [hidelinks] {hyperref}


\newcommand{\dir} [1] [] {#1} 
\newcommand{\todoref}{\todo{ref}}

% Title Page
\title{Concurrency Patterns in SCOOP}
\author{Roman Schmocker}


\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\section{Introduction}

Due to the advent of multicore processors, concurrent programming has become an important part in software engineering.
Dealing with parallelism isn't easy however.
There are many pitfalls, such as race conditions and deadlocks.

In practice programmers have learned to avoid tricky concurrency problems with the use of some well-known patterns \todo{is it ok to take introduction from project plan almost word for word?}.
These patterns are often shipped with the standard library of the language, such that users rarely have to implement them.

The Eiffel language \todo{ref} has a new concurrency extension called SCOOP, which stands for Simple Concurrent Object-Oriented Programming.
SCOOP simplifies concurrent programming a lot and eliminates one source of errors completely, namely race conditions \todo{ref}.
However, there is little experience on how to implement popular concurrency patterns, such as a worker pool, in SCOOP.

The goal of this thesis is therefore to implement a standard library for concurrency patterns in Eiffel.

\subsection{Overview}

Section \ref{sec:pattern_overview} introduces a list of concurrency patterns which we found and categorized by studying literature and the standard libraries.
A brief introduction to the SCOOP model is given in Section \ref{sec:scoop-model}.
Section \ref{sec:scoop-challenges} describes some challenges when programming in SCOOP and how to solve them.
The latter two sections may be interesting for programmers with experience in threaded programming, but who wish to learn SCOOP.

The focus of Section \ref{sec:library} describes the goals and concepts of the concurrency patterns library.
It also provides a brief overview over the available modules, and which patterns are implemented by which modules.

A detailed explanation over the individual modules is provided by Section \ref{sec:modules}.
Finally, Section \ref{sec:evaluation} provides a small performance evaluation of the library.

\section {Pattern overview}
\label{sec:pattern_overview}
% Overview of all patterns, maybe tabular

\input{pattern_list}

\section {The SCOOP model}
\label {sec:scoop-model}
% Introduction, differences to Java etthirdc... (short)

SCOOP is an extension to the Eiffel language \todo{ref} that aims to make concurrent programming easier.
The basic idea is that every object can only be accessed by exactly one computational unit.
This unit is called processor, or handler of an object.

The keyword \lstinline!separate! is used to indicate that an object may be handled by a processor with respect to the handler for \lstinline!Current!.
Calls to a separate object (``separate calls'') then correspond to sending a message to the foreign processor.
There are two types of separate calls: synchronous and asynchronous.
If the called feature returns a result, the call is synchronous, which means that the current processor has to wait for the foreign processor to finish its task.
An asynchonous call happens when the feature is a command, i.e. not returning any result.
In that case, both processors can proceed concurrently.

A separate call is only allowed if the target of the call is ``controlled''.
Having an object controlled means that the user has exclusive access to that object - in that sense controlling an object corresponds a bit to locking in other languages.
In order to control an object it has to appear as a formal argument in the enclosing routine.

SCOOP guarantees that all messages sent by the current processor are handled in the foreign processor in the correct order.
A consequence of this and the exclusive access guarantee is that within a feature body, a separate object can be treated as if it were in a sequential program.
This is the reason why the SCOOP model is so simple: 
It allows reasoning about a feature body without the need to consider all possible interleavings of two parallel executions.

To create a new processor, one has to use the creation instruction on an object which is declared as separate.
The new processor is then initialized automatically, and the new object is handled by the new processor.

\todo{sortly describe wait conditions.}

There are many advantages to the SCOOP model, such as easier reasoning and absence of data races, but it also has some shortcomings.
One of them is the fact that it's a bit tedious to write SCOOP programs.
Due to the fact that every call to a separate object needs to be controlled, a user often has to write little helper functions that take a separate reference and just perform a single call on it.
It also has some performance problems, because encapsulating a separate call in a message may be expensive (especially for small functions like array access).
Additionally, a processor is currently implemented as an operating system thread, and creating them is an expensive operation that involves context switches.
The SCOOP model however encourages the creation of many processors, which is not ideal for performance reasons.

\section{Challenges in SCOOP}
\label{sec:scoop-challenges}
% This section describes recurring challenges in SCOOP and how to solve them...

\subsection{Object migration}
\label{sec:object-migration}

Passing data from one processor to another is often necessary when programming in SCOOP.
The most obvious example is the producer/consumer pattern, but it also applies to other situations where one just wants to provide some arguments to an asynchronous command.

\todo{what about expanded objects?}

There are basically three ways to safely pass reference objects from a sender to a receiver processor.
The first and easiest solution is to create data on its own, separate processor: 
\begin{lstlisting}
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: separate ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This approach is conceptually easy, but it has the drawback that it's not very efficient, especially when the argument object is very small.
We'll call this solution the Data Processor approach.

Another solution is to create the object on the same handler as the sender object:
\begin{lstlisting}
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This solution (the Lock Passing approach) looks almost like the first one - the only change is a missing separate keyword.
However, it's semantics are radically different:

\begin{itemize}
 \item Due to the lock passing mechanism \todo{ref: piotr and eiffel docs} the feature \lstinline!do_something! is executed synchronously, i.e. the sender has to wait for it to finish.
 \item \lstinline!RECEIVER! can't access the argument object any more after the call \lstinline!do_something! is finished.
 This is because \lstinline!SENDER! will continue it's execution, and an attempt to lock the argument object again will probably result in starvation of the sender processor.
 \item Compared to the first approach, no new processor is created.
\end{itemize}

The last method makes use of a special SCOOP function called \lstinline!import!:
\begin{lstlisting}
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.receive_args (args)
      a_receiver.do_something
    end
end

class RECEIVER feature
  
  received: ANY
  
  receive_args (args: separate ANY)
      -- Receive some arguments
    do
      received := import (args)
    end

  do_something
      -- Perform some operation.
    do
      print (received)
    end
end
\end{lstlisting}
The \lstinline!import! feature copies its argument object, along with all non-separate references, to the local processor.
It is somewhat similar to \lstinline!{ANY}.deep_copy!, except that it doesn't clone separate references.

This import solution has several advantages.
There's no need for a new processor, and the receiver can keep the data and do the operation asynchronously.
The drawback however is that the data needs to be copied.
However, for small data items this is actually faster than creating a new thread.

Note that \lstinline!receive_args! is executed synchronously just like in the Lock Passing approach.
Therefore, to execute \lstinline!do_something! asynchronously, it has to be divided into an execution and argument receiving part.

The feature \lstinline!import! was first described in \todo{ref: Piotr}, but unfortunately it is not implemented in current SCOOP.
It is possible however to implement it manually with some user support.

\subsection{Processor communication}
\label{sec:processor-communication}
% Problem that two concurrent, active processors can't communicate. Example downloader task.
% Solution: a third, passive processor with a shared data structure.


It is often necessary that two threads need to communicate with each other.
One example would be a user interface with a background downloader task.
The user interface needs to be able to cancel the downloader, and the downloader needs to inform the GUI that it is finished.

In SCOOP this is not easily done.
Both processors are performing a long-running execution, which doesn't allow other processors to do separate calls on them.
Specifically, the GUI processor is in a main loop to receive input and repaint the window, whereas the downloader task is busy receiving chunks of data.

The solution to this kind of problem is to introduce a third processor which is ``passive'', meaning that it doesn't have a task to perform and only waits for incoming requests.
This third processor is known to the other two, ``active'' processors, and it contains all the attributes which are necessary for communication.
In our example this means that the ``passive'' processor takes care of an object with an \lstinline!is_cancelled! and \lstinline!is_finished! boolean flag.
The ``active'' processors then regularly need to check the status of these flags.

\subsection{Processor termination}
\label{sec:processor-termination}
% Problem: how to stop consumer waiting on empty buffer.
% Solution: Query is_stopped in shared buffer.

When an application needs to be shut down, it is necessary to stop any running threads.
Sometimes this can be done via the ``passive'' third processor as seen in Section \ref{sec:processor-communication}.
However, the active processor may be stuck in a wait condition.

One example of this could be the producer/consumer pattern, where a consumer is waiting for a buffer to become non-empty.
If all producers are already terminated, the consumer never gets the chance to break out of this wait condition and therefore it can't terminate successfully.

The solution is to add a query \lstinline!is_stop_requested! right inside the shared buffer, and to adapt to adapt the wait condition to include the stop request:

\begin{lstlisting}
class
  CONSUMER

feature -- Status report

  buffer: separate BUFFER
  
  last_item: INTEGER
  
  is_stopped: BOOLEAN
  
feature -- Basic operations
  
   start
      -- Start the main loop
    do
      from 
      until 
	is_stopped
      loop
	fetch (buffer)
	if not is_stopped then
	  -- do something
	end
      end
    end
  
feature -- Implementation

  fetch (buf: separate BUFFER)
      -- Get the next item from `buf'.
    require
      not buf.is_empty or buf.is_stop_requested
    do
      if buf.is_stop_requested then
	is_stopped := True
      else
	last_item := buf.item
	buf.remove
      end
    end
end
\end{lstlisting}


\section {Library}
\label{sec:library}

% The library is designed as a set of modules which simplify concurrent programming in SCOOP.
% Section ~\ref{sec:tutorial} explains how to use the library for commonly used patterns,
% while Section ~\ref{sec:modules} concentrates on the design of the library itself.
 
\subsection{Goals}
\label{sec:goals}

The goal of the library is to provide a set of tools that simplify programming in SCOOP.
Specifically, we want to provide implementations for common concurrency patterns like the worker pool.
The result should be a new SCOOP library similar to the standard concurrency libraries in Java \todoref or C\# \todoref.

The library was developed with the following design goals:

\begin{itemize}
 \item Avoid common SCOOP pitfalls, like deadlock problems, starvation of a processor, or unintentional lock passing.
 \item Shield the user from having to write ``small wrapper'' features, i.e. features that need to be written just to lock an object for a separate call.
 \item Reduce the overhead of thread creation, especially for concurrent programs that involve dealing with a lot of small separate objects.
\end{itemize}

\subsection{Concepts}

This section describes the core concepts of the library: Import and Separate Proxies.
The import concept deals with the problem of how to pass data from one processor to another.
It is usefol to achieve design goal \#3\todoref and to some extent \#1\todoref in Section \ref{sec:goals}.

The Separate Proxy is a pattern to hide separate references behind a proxy object.
It provides a solution to design goal \#2 \todoref.

\subsubsection{Import}
\label{sec:concepts:import}
% Describe how to use it, i.e. generic parameter and importer objects, and the fact that you can select between import and no import.

The import concept is a central part of the library.
It was developed to let users choose between two object passing strategies, namely the Data Processor \todoref and the Import \todoref approach.

The main class is the deferred \lstinline!CP_IMPORT_STRATEGY [G]!, which has the simple interface:
\lstinputlisting [firstline=7] {../../library/import/cp_import_strategy.e}

The class has two descendants: \lstinline!CP_NO_IMPORTER[G]! can be used for the Data Processor strategy. 
It just perform a reference copy of the object.
The class \lstinline!CP_IMPORTER [G]! on the other hand narrows the return type of \lstinline!import! to a non-separate \lstinline!G!, meaning that it actually performs an import.

As there's no general-purpose import feature available at the moment, a user has to implement his own feature for every class that needs to be imported.
Descendants of \lstinline!CP_IMPORTER! simplify this task and provide predefined implementations for some standard classes such as \lstinline!STRING!.
Those mechanisms are described in detail in Section \ref{sec:modules:import}.

\todo{elaborate more, explain less specific}
Other components of the library often use the import module via bounded genericity.
The class \lstinline!CP_QUEUE! for example has the ability to import objects, and it's class header looks like this:
\begin{lstlisting}
class CP_QUEUE
  [G, IMPORTER -> CP_IMPORT_STRATEGY [G] create default_create end]
feature
  ...
end
\end{lstlisting}
That way a user can decide on the precise semantics of the import strategy by just declaring the right type.



\subsubsection{Separate Proxy}

The separate proxy pattern provides a nice interface to a separate object, by providing a processor-local proxy which hides the separate reference.
It is applied to all classes in the library which are meant to be shared among processors, i.e. which are usually accessed through a separate reference.

The pattern consists of three classes:
\begin{enumerate} [label=(\arabic*)]
 \item\label{item:sep-proxy:first} The class for the actual separate object.
 \item\label{item:sep-proxy:second} A class that provides helper functions to access a separate object of type \ref{item:sep-proxy:first}, usually with the ending \lstinline!_UTILS!.
 \item\label{item:sep-proxy:third} A proxy class with the a similar interface as \ref{item:sep-proxy:first}, usually ending on \lstinline!_PROXY!.
    Using \ref{item:sep-proxy:second}, the proxy forwards all calls to an object of type \ref{item:sep-proxy:first}.
\end{enumerate}

\todo{A little diagram showing the class relations.}

It is possible to add a fourth, deferred class that just defines the interface for \ref{item:sep-proxy:first} and \ref{item:sep-proxy:third}.
However, there's an inconsistency: 
Any precondition in \ref{item:sep-proxy:first} which references \lstinline!Current! needs to be converted to a wait condition in \ref{item:sep-proxy:third} that references \ref{item:sep-proxy:first}.
Furthermore, not all features in the business class may be necessary in the proxy, and the proxy itself may add some more features such as compound actions.

Unfortunately this pattern cannot be fully turned into a module, because it's highly dependent on the precise interface of the business class.
There is however some support in the library: 
\lstinline!CP_PROXY! defines the creation procedure and the attributes \lstinline!subject! for a business class object and \lstinline!utils! for a helper object.

\todo{reference to appendix}

\subsection {Module overview}
\label{sec:module-overview}

The library consists of several modules which implement some of the patterns described in the overview (Section \ref{sec:pattern_overview}).

One of the most basic modules is the Import module in \dir{library/import}.
It implements the Import pattern \todoref and is at the same time one of the core concepts of the library.

The queue module in \dir{library/queue} implements the Prdocuer / Consumer pattern \todoref.
It depends upon the import module.

The process module in \dir{library/process} provides skeleton classes for objects with a main loop.
It provides implementations for the Active Object \todoref, Self Asynch \todoref and Timer: Periodic \todoref patterns.

The worker pool module in \dir{library/worker\_pool} implements the Worker Pool pattern \todoref.
It depends on the import, queue and process module.

The future module is located in \dir{library/executor} and \dir{library/promise}.
It provides an implementation for the Future \todoref as well as the Executor framework \todoref.

The Timer: InvokeLater pattern \todoref is implemented by a single class \lstinline!CP_DEFAULT_TASK! in \dir{libary/util}.

% \todo{Describe available modules, which patterns they're implementing, and which modules they depend upon.}


\input{modules}

\section{Evaluation}
\label {sec:evaluation}

To evaluate the library we implemented a small performance benchmark.
We implemented the Gaussian elimination algorithm \todoref in three different ways: sequentially, with SCOOP only, and using the future module \todoref from the library.
We chose to test the future pattern because it indirectly also measures many other parts of the library, like the worker pool or import mechanism.

We ran the tests with randomly generated matrices and each test was repeated 5 times.
A test matrix was square and its order always a power of two in the range from 32 to 1024.
Additionally, there was one more column for the result vector in the system of linear equations.
The test system was a quad-core AMD Phenom II X4 955 processor with 6 GB of RAM.

The results of the tests are shown in the table below:

\todo{calculate averages, insert table, write warning about special data size in Caption.}

From the results we can get several observations:

\begin{itemize}
 \item The raw SCOOP solution fails for the biggest matrix. 
 This is because it uses more than the maximum number of processors, and it's a known bug \todo{ref: https://docs.eiffel.com/book/solutions/scoop-implementation}.
 The library solution doesn't suffer from this problem because it's using a fixed amount of processors.
 \item The library is a lot faster than the raw SCOOP solution on large data sets.
 \item For smaller data sets, raw SCOOP beats the library.
 \item Sequential execution is a lot faster than SCOOP.
\end{itemize}

The last observation is probably the most fundamental.
The SCOOP runtime really needs to be improved in order to make it competitive to threaded systems, or even sequential ones.
Fortunately an improved version \todo{ref: Scott's thesis} is being developed at the time of writing.
It will be integrated into a future EiffelStudio release.

Another improvement which might be useful for the library is the Passive Processor concept \todoref.
This might be useful especially for the Future module, as both the worker pool and the promise object could be declared passive.

\section{Conclusion}
% Explain what is the impact of the selected patterns, and why they were selected.
% Say that import was developed to overcome language limitation
% Also: Future work


In this thesis we've worked out many methods that simplify concurrent programming in one way or another.
We've done a survey of concurrency patterns and described them in an extensive list.
This list can be used by anyone searching for a particular pattern.

From this list we selected some patterns which we thought to be especially useful.
The selection was based on the study of other concurrency libraries as well as some input from the CME research group \todoref.

The seleted patterns were then implemented and are now available as a new Eiffel library.
Besides the actual pattern implementations, this library also provides some workarounds for current SCOOP limitations, such as the missing import statement.

Performance measurements for the Future pattern implementation showed that the library is actually faster foe large data sets and uses less threads than the native SCOOP approach.

Finally we also describe some challenges when programming in SCOOP, and how they can be solved.
This is especially useful to programmers new to SCOOP, but which may have experience in concurrent programming with threads.

\subsection{Future work}

The library provides several opportunities for future work.

\begin{description}
 \item [More patterns] The library can be extended with further patterns.
 It may be useful to include Pipeline or Dataflow network.
 \item [Separate Proxies] It may be useful to apply the Separate Proxy patterns to some EiffelBase classes, such as \lstinline!ARRAYED_LIST!, \lstinline!HASH_TABLE! or \lstinline!ROUTINE!.
 \item [Separate Proxy Wizard] The creation of a separate proxy can be mostly automated with a wizard.
% \item [EiffelVision support] \todo{maybe?}
% \item [Agent integration] \todo{maybe?}
 \item [Concurrent Datastructures] Sometimes it may be useful to have truly concurrent data structures for performance reasons.
 Pure SCOOP doesn't allow to do this, but it can be done in a C library. 
\end{description}

A important step is to improve SCOOP itself.
There are various ways to do it, most of them related to performance improvements.

\begin{description}
 \item [Faster runtime] The SCOOP runtime needs to become faster. 
 This is currently being developed \todoref.
 \item [Native Import] A native SCOOP import feature is a good tool to deal with a lot of small objects.
 It will also make the library code and usage simpler, as the manual import workaround can be removed.
 \item [Passive Processors] The passive processors concept could be integrated into EiffelStudio.
 It can make a big performance improvement to situations where one needs to pass data from one processor to another.
 \item [Separate references] The handling of separate references should become more simple.
 At the moment a programmer is forced to write a lot of small, annoying features to perform separate calls.
 Some syntactic sugar would really be helpful.
\end{description}



\begin{appendix}
\input{tutorial}
\input{separate_proxy}
\end{appendix}

\todos

\end{document}          
