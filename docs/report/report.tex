\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

\usepackage{pattern}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{todo}
\usepackage{appendix}

\newcommand{\dir} [1] [] {#1} 
\newcommand{\todoref}{\todo{ref}}

% Title Page
\title{Concurrency Patterns in SCOOP}
\author{Roman Schmocker}


\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\section{Introduction}

Due to the advent of multicore processors, concurrent programming has become an important part in software engineering.
Dealing with parallelism isn't easy however.
There are many pitfalls, such as race conditions and deadlocks.

In practice programmers have learned to avoid tricky concurrency problems with the use of some well-known patterns \todo{is it ok to take introduction from project plan almost word for word?}.
These patterns are often shipped with the standard library of the language, such that users rarely have to implement them.

The Eiffel language \todo{ref} has a new concurrency extension called SCOOP, which stands for Simple Concurrent Object-Oriented Programming.
SCOOP simplifies concurrent programming a lot and eliminates one source of errors completely, namely race conditions \todo{ref}.
However, there is little experience on how to implement popular concurrency patterns, such as a worker pool, in SCOOP.

The goal of this thesis is therefore to implement a standard library for concurrency patterns in Eiffel.

Section \todoref introduces a list of concurrency patterns which we found and categorized by studying literature and the standard libraries.
A brief introduction to the SCOOP model is given in Section \todoref.
The focus of Section \todoref is to describe the library itself.
The first part contains an API tutorial, whereas the second part focuses on the design of the library itself.
Finally, Section \todoref describes some interesting patterns with special problems and the solutions to them.

\section {Pattern overview}
% Overview of all patterns, maybe tabular

\input{pattern_list}

\section {The SCOOP model}
% Introduction, differences to Java etthirdc... (short)

SCOOP is an extension to the Eiffel language \todo{ref} that aims to make concurrent programming easier.
The basic idea is that every object can only be accessed by exactly one computational unit.
This unit is called processor, or handler of an object.

The keyword \lstinline!separate! is used to indicate that an object may be handled by a processor with respect to the handler for \lstinline!Current!.
Calls to a separate object (``separate calls'') then correspond to sending a message to the foreign processor.
There are two types of separate calls: synchronous and asynchronous.
If the called feature returns a result, the call is synchronous, which means that the current processor has to wait for the foreign processor to finish its task.
An asynchonous call happens when the feature is a command, i.e. not returning any result.
In that case, both processors can proceed concurrently.

A separate call is only allowed if the target of the call is ``controlled''.
Having an object controlled means that the user has exclusive access to that object - in that sense controlling an object corresponds a bit to locking in other languages.
In order to control an object it has to appear as a formal argument in the enclosing routine.

SCOOP guarantees that all messages sent by the current processor are handled in the foreign processor in the correct order.
A consequence of this and the exclusive access guarantee is that within a feature body, a separate object can be treated as if it were in a sequential program.
This is the reason why the SCOOP model is so simple: 
It allows reasoning about a feature body without the need to consider all possible interleavings of two parallel executions.

To create a new processor, one has to use the creation instruction on an object which is declared as separate.
The new processor is then initialized automatically, and the new object is handled by the new processor.

There are many advantages to the SCOOP model, such as easier reasoning and absence of data races, but it also has some shortcomings.
One of them is the fact that it's a bit tedious to write SCOOP programs.
Due to the fact that every call to a separate object needs to be controlled, a user often has to write little helper functions that take a separate reference and just perform a single call on it.
It also has some performance problems, because encapsulating a separate call in a message may be expensive (especially for small functions like array access).
Additionally, a processor is currently implemented as an operating system thread, and creating them is an expensive operation that involves context switches.
The SCOOP model however encourages the creation of many processors, which is not ideal for performance reasons.

\section{Challenges in SCOOP}
% This section describes recurring challenges in SCOOP and how to solve them...

\subsection{Object migration}
\label{sec:object-migration}

Passing data from one processor to another is often necessary when programming in SCOOP.
The most obvious example is the producer/consumer pattern, but it also applies to other situations where one just wants to provide some arguments to an asynchronous command.

\todo{what about expanded objects?}

There are basically three ways to safely pass reference objects from a sender to a receiver processor.
The first and easiest solution is to create data on its own, separate processor: 
\begin{lstlisting}
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: separate ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This approach is conceptually easy, but it has the drawback that it's not very efficient, especially when the argument object is very small.
We'll call this solution the Data Processor approach.

Another solution is to create the object on the same handler as the sender object:
\begin{lstlisting}
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This solution (the Lock Passing approach) looks almost like the first one - the only change is a missing separate keyword.
However, it's semantics are radically different:

\begin{itemize}
 \item Due to the lock passing mechanism \todo{ref: piotr and eiffel docs} the feature \lstinline!do_something! is executed synchronously, i.e. the sender has to wait for it to finish.
 \item \lstinline!RECEIVER! can't access the argument object any more after the call \lstinline!do_something! is finished.
 This is because \lstinline!SENDER! will continue it's execution, and an attempt to lock the argument object again will probably result in starvation of the sender processor.
 \item Compared to the first approach, no new processor is created.
\end{itemize}

The last method makes use of a special SCOOP function called \lstinline!import!:
\begin{lstlisting}
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.receive_args (args)
      a_receiver.do_something
    end
end

class RECEIVER feature
  
  received: ANY
  
  receive_args (args: separate ANY)
      -- Receive some arguments
    do
      received := import (args)
    end

  do_something
      -- Perform some operation.
    do
      print (received)
    end
end
\end{lstlisting}
The \lstinline!import! feature copies its argument object, along with all non-separate references, to the local processor.
It is somewhat similar to \lstinline!{ANY}.deep_copy!, except that it doesn't clone separate references.

This import solution has several advantages.
There's no need for a new processor, and the receiver can keep the data and do the operation asynchronously.
The drawback however is that the data needs to be copied.
However, for small data items this is actually faster than creating a new thread.

Note that \lstinline!receive_args! is executed synchronously just like in the Lock Passing approach.
Therefore, to execute \lstinline!do_something! asynchronously, it has to be divided into an execution and argument receiving part.

The feature \lstinline!import! was first described in \todo{ref: Piotr}, but unfortunately it is not implemented in current SCOOP.
It is possible however to implement it manually with some user support.

\subsection{Processor communication}
\label{sec:processor-communication}
% Problem that two concurrent, active processors can't communicate. Example downloader task.
% Solution: a third, passive processor with a shared data structure.


It is often necessary that two threads need to communicate with each other.
One example would be a user interface with a background downloader task.
The user interface needs to be able to cancel the downloader, and the downloader needs to inform the GUI that it is finished.

In SCOOP this is not easily done.
Both processors are performing a long-running execution, which doesn't allow other processors to do separate calls on them.
Specifically, the GUI processor is in a main loop to receive input and repaint the window, whereas the downloader task is busy receiving chunks of data.

The solution to this kind of problem is to introduce a third processor which is ``passive'', meaning that it doesn't have a task to perform and only waits for incoming requests.
This third processor is known to the other two, ``active'' processors, and it contains all the attributes which are necessary for communication.
In our example this means that the ``passive'' processor takes care of an object with an \lstinline!is_cancelled! and \lstinline!is_finished! boolean flag.
The ``active'' processors then regularly need to check the status of these flags.

\subsection{Processor termination}
\label{sec:processor-termination}
% Problem: how to stop consumer waiting on empty buffer.
% Solution: Query is_stopped in shared buffer.

When an application needs to be shut down, it is necessary to stop any running threads.
Sometimes this can be done via the ``passive'' third processor as seen in Section \ref{sec:processor-communication}.
However, the active processor may be stuck in a wait condition.

One example of this could be the producer/consumer pattern, where a consumer is waiting for a buffer to become non-empty.
If all producers are already terminated, the consumer never gets the chance to break out of this wait condition and therefore it can't terminate successfully.

The solution is to add a query \lstinline!is_stop_requested! right inside the shared buffer, and to adapt to adapt the wait condition to include the stop request:

\begin{lstlisting}
class
  CONSUMER

feature -- Status report

  buffer: separate BUFFER
  
  last_item: INTEGER
  
  is_stopped: BOOLEAN
  
feature -- Basic operations
  
   start
      -- Start the main loop
    do
      from 
      until 
	is_stopped
      loop
	fetch (buffer)
	if not is_stopped then
	  -- do something
	end
      end
    end
  
feature -- Implementation

  fetch (buf: separate BUFFER)
      -- Get the next item from `buf'.
    require
      not buf.is_empty or buf.is_stop_requested
    do
      if buf.is_stop_requested then
	is_stopped := True
      else
	last_item := buf.item
	buf.remove
      end
    end
end
\end{lstlisting}
% class
%   BUFFER
%   
% feature -- Status report
%   
%   is_stop_requested: BOOLEAN
%       -- Do the consumers need to stop?
%       
%   is_empty: BOOLEAN
%       -- Is the buffer empty?
% 
% end
% \end{lstlisting}


\section {Library}

% The library is designed as a set of modules which simplify concurrent programming in SCOOP.
% Section ~\ref{sec:tutorial} explains how to use the library for commonly used patterns,
% while Section ~\ref{sec:modules} concentrates on the design of the library itself.
 
\subsection{Goals}
\label{sec:goals}

The goal of the library is to provide a set of tools that simplify programming in SCOOP.
Specifically, we want to provide implementations for common concurrency patterns like the worker pool.
The result should be a new SCOOP library similar to the standard concurrency libraries in Java \todoref or C\# \todoref.

The library was developed with the following design goals:

\begin{itemize}
 \item Avoid common SCOOP pitfalls, like deadlock problems, starvation of a processor, or unintentional lock passing.
 \item Shield the user from having to write ``small wrapper'' features, i.e. features that need to be written just to lock an object for a separate call.
 \item Reduce the overhead of thread creation, especially for concurrent programs that involve dealing with a lot of small separate objects.
\end{itemize}

\subsection{Concepts}

This section describes the core concepts of the library: Import and Separate Proxies.
The import concept deals with the problem of how to pass data from one processor to another.
It is usefol to achieve design goal \#3\todoref and to some extent \#1\todoref in Section \ref{sec:goals}.

The Separate Proxy is a pattern to hide separate references behind a proxy object.
It provides a solution to design goal \#2 \todoref.

\subsubsection{Import}
\label{sec:concepts:import}
% Describe how to use it, i.e. generic parameter and importer objects, and the fact that you can select between import and no import.

The import concept is a central part of the library.
It was developed to let users choose between two object passing strategies, namely the Data Processor \todoref and the Import \todoref approach.

The main class is the deferred \lstinline!CP_IMPORT_STRATEGY [G]!, which has the simple interface:
\lstinputlisting [firstline=7] {../../library/import/cp_import_strategy.e}

The class has two descendants: \lstinline!CP_NO_IMPORTER[G]! can be used for the Data Processor strategy. 
It just perform a reference copy of the object.
The class \lstinline!CP_IMPORTER [G]! on the other hand narrows the return type of \lstinline!import! to a non-separate \lstinline!G!, meaning that it actually performs an import.

As there's no general-purpose import feature available at the moment, a user has to implement his own feature for every class that needs to be imported.
Descendants of \lstinline!CP_IMPORTER! simplify this task and provide predefined implementations for some standard classes such as \lstinline!STRING!.
Those mechanisms are described in detail in Section \ref{sec:modules:import}.

\todo{elaborate more, explain less specific}
Other components of the library often use the import module via bounded genericity.
The class \lstinline!CP_QUEUE! for example has the ability to import objects, and it's class header looks like this:
\begin{lstlisting}
class CP_QUEUE
  [G, IMPORTER -> CP_IMPORT_STRATEGY [G] create default_create end]
feature
  ...
end
\end{lstlisting}
That way a user can decide on the precise semantics of the import strategy by just declaring the right type.



\subsubsection{Separate Proxy}

The separate proxy pattern provides a nice interface to a separate object, by providing a processor-local proxy which hides the separate reference.
It is applied to all classes in the library which are meant to be shared among processors, i.e. which are usually accessed through a separate reference.

The pattern consists of three classes:
\begin{enumerate} [label=(\arabic*)]
 \item\label{item:sep-proxy:first} The class for the actual separate object.
 \item\label{item:sep-proxy:second} A class that provides helper functions to access a separate object of type \ref{item:sep-proxy:first}, usually with the ending \lstinline!_UTILS!.
 \item\label{item:sep-proxy:third} A proxy class with the a similar interface as \ref{item:sep-proxy:first}, usually ending on \lstinline!_PROXY!.
    Using \ref{item:sep-proxy:second}, the proxy forwards all calls to an object of type \ref{item:sep-proxy:first}.
\end{enumerate}

\todo{A little diagram showing the class relations.}

It is possible to add a fourth, deferred class that just defines the interface for \ref{item:sep-proxy:first} and \ref{item:sep-proxy:third}.
However, there's an inconsistency: 
Any precondition in \ref{item:sep-proxy:first} which references \lstinline!Current! needs to be converted to a wait condition in \ref{item:sep-proxy:third} that references \ref{item:sep-proxy:first}.
Furthermore, not all features in the business class may be necessary in the proxy, and the proxy itself may add some more features such as compound actions.

Unfortunately this pattern cannot be fully turned into a module, because it's highly dependent on the precise interface of the business class.
There is however some support in the library: 
\lstinline!CP_PROXY! defines the creation procedure and the attributes \lstinline!subject! for a business class object and \lstinline!utils! for a helper object.

\todo{reference to appendix}

\subsection {Architecture}
\label{sec:modules}



The library consists of several modules, which often build on each other.
\todo{Describe available modules, which patterns they're implementing, and which modules they depend upon.}

\section{Library implementation}

\subsubsection{Import}
\label{sec:modules:import}

The import module implements the SCOOP \lstinline!import! feature, which should be built-in but isn't implemented at the moment.
It is one of the most important modules of the library.
It consists of all classes in the directory \dir{library/import}.

The basic concepts on how to use the module is described in Section \ref{sec:concepts:import}.
This section only describes the descendants of \lstinline!CP_IMPORTER!, which provide predefined import functions for some types and support for writing an import feature manually.

The deferred class \lstinline!CP_IMPORTER [G]! defines the basic interface for a manually defined import feature.
To define an import feature for an arbitrary type, e.g. \lstinline!STRING!, one has to inherit from \lstinline!CP_IMPORTER [STRING]! and provide an implementation for the deferred \lstinline!import! feature.

% The main class is the deferred \lstinline!CP_IMPORT_STRATEGY [G]!, which has the simple interface:
% \lstinputlisting [firstline=7] {../../library/import/cp_import_strategy.e}
% By implementing \lstinline!import!, descendants can decide if an object of type \lstinline!G! shall be imported and if yes, how it's done.
% 
% The class \lstinline!CP_NO_IMPORTER[G]! is the default class to disable import and just perform a reference copy.
% Every other importer can inherit from \lstinline!CP_IMPORTER!, which narrows the return type of \lstinline!import! to a non-separate \lstinline!G!.

Because writing an extra importer for every importable object may be tedious, there's a support class \lstinline!CP_IMPORTABLE!:

\lstinputlisting [firstline=7] {../../library/import/cp_importable.e}

That way an import function can be written right inside the class that needs to be imported.

There are two classes which can be used for \lstinline!CP_IMPORTABLE! objects: \lstinline!CP_DYNAMIC_TYPE_IMPORTER! and \lstinline!CP_STATIC_TYPE_IMPORTER!.
The latter uses bounded genericity to directly create an object of type \lstinline!G!.
This has the drawback however that the type is statically \lstinline!G!, even if the argument to \lstinline!import! was of a subtype of \lstinline!G!.

The \lstinline!CP_DYNAMIC_TYPE_IMPORTER! tries to avoid this problem by using reflection.
This introduces a new problem with respect to void safety however, as the new object will not be initialized.
Therefore it is strongly advised to declare \lstinline!make_from_separate! as a creation procedure for every descendant of \lstinline!CP_IMPORTABLE!.

Another problem of the \lstinline!CP_DYNAMIC_TYPE_IMPORTER! are the invariants of an object.
There's a short time interval between the creation of an object (using reflection) and the call to \lstinline!{CP_IMPORTABLE}.make_from_separate! where the invariants are broken.
Due to this, it is not possible to have invariants in a class that will be imported using the dynamic type importer.

In the future, there will hopefully exist an import routine natively supported by the SCOOP runtime.
In that case \lstinline!CP_IMPORTER! can be made effective and use the native import, and all its descendants will become obsolete.


% Other components of the library often use the import module via bounded genericity.
% The class \lstinline!CP_QUEUE! for example has the ability to import objects, and it's class header looks like this:
% \begin{lstlisting}
% class CP_QUEUE
%   [G, IMPORTER -> CP_IMPORT_STRATEGY [G] create default_create end]
% feature
%   ...
% end
% \end{lstlisting}
% That way a user can decide on the precise semantics of the import strategy by just declaring the right type.

\subsection{Queue}

The queue module is a simple queue implementation in class \lstinline!CP_QUEUE!.
It is mostly used to easily implement the producer / consumer pattern in a concurrent context.


The pattern itself is all about data migration as described in Section \ref{sec:object-migration}.
Therefore the queue module makes heavy use of the import mechanism.
This means that, along with a generic argument for the data type, it is also necessary to provide a \lstinline!CP_IMPORT_STRATEGY!.
The import strategy then basically ``teaches'' the queue how to import a given object.

Internally, \lstinline!CP_QUEUE! uses an \lstinline!ARRAYED_LIST! to store its elements.

As the \lstinline!CP_QUEUE! is intended to be used as a separate object, the module provides support classes that implement the Separate Proxy pattern.

% It uses the separate proxy pattern, which means that it consists of three classes:
% 
% \begin{itemize}
%  \item \lstinline!CP_QUEUE!
%  \item \lstinline!CP_QUEUE_UTILS!
%  \item \lstinline!CP_QUEUE_PROXY!
% \end{itemize}
% 
% The first one provides the actual queue, but internally it just relies on \lstinline!ARRAYED_QUEUE!.
% The class \lstinline!CP_QUEUE_PROXY! can be used to access such a shared, separate queue without having to deal with separate references.
% 
% The interesting thing about this queue implementation is that it makes use of the import module.
% Along with the generic argument \lstinline!G! you can also provide an \lstinline!CP_IMPORT_STRATEGY [G]!.
% The import then happens automatically in both the queue and its proxy.

% \subsubsection{Producer / Consumer}
% 
% 
% The producer / consumer is a very popular pattern in concurrent programming, and it is a building block for other patterns like pipeline as well.
% The basic idea is to have a shared, concurrent buffer.
% Producer threads put new items into the buffer, whereas consumer threads remove items from this buffer.
% 
% \todo{BON-style diagram and ``migration graphics''}
% 
% In threaded systems, this buffer is usually accessed by several threads at the same time.
% Careful synchronization has to be enforced to ensure that the buffer remains in a consistent state.
% The data items on the other hand ``migrate'' from a producer to the buffer, and then to exactly one consumer.
% They therefore don't need to be thread-safe as long as the producer promises never to touch the item again.
% 
% In SCOOP things look a bit different however.
% Due to the exclusive access guarantee it is not necessary to establish a synchronization policy.
% The downside however is a loss of potential concurrency when a producer and a consumer accesses the buffer simultaneously, but this is a minor problem.
% 
% The main problem in SCOOP are the data items, especially if they are not of an expanded type.
% If the object is created on the producer processor, then the consumer needs to control the producer in order to get access to the object.
% This is clearly a situation that we want to avoid, because it couples the producer and consumer in a vicious hidden way, and the whole point of the producer / consumer pattern is to decouple the two.
% 
% A nice solution would be if it's somehow possible to migrate data items, like it's done in threaded languages.
% However, this is not possible with the current semantics of SCOOP, because an object always stays on the processor it was created on.
% 
% One approach to solve this problem is to create a new processor for every data item.
% This actually works, but it can be very slow, especially for a lot of small data items.
% There are two reasons for this:
% First, every SCOOP processor is mapped to an operating system thread, therefore creating a new processor involves creating a new thread which is an expensive operation.
% The second reason is the overhead of separate calls itself.
% This has to be paid every time the consumer wants to access the separate object.
% 
% Another problem of this approach is related to ease of programming.
% Dealing with a separate object can be very annoying, because you need to write small wrapper functions for every little feature call.
% 
% \begin{lstlisting}
% class
%   CONSUMER
% 
% feature -- Basic operations
%   
%   retrieve
%       -- Retrieve a string and print its length.
%     local
%       l_string: separate STRING
%     do
%       l_string := buffer_consume (a_queue)
%       print (string_count (l_string))
%     end
%     
% feature {NONE} -- Implementation
%   
%   buffer: separate BUFFER [STRING]
% 
%   buffer_consume (a_buffer: like buffer): separate STRING
%       -- An annoying small wrapper function for a buffer.
%     do
%       Result := a_buffer.item
%       a_buffer.remove
%     end
%     
%   string_count (a_string: separate STRING): INTEGER
%       -- An annoying small wrapper function for a string.
%     do
%       Result := a_string.count
%     end
% end
% \end{lstlisting}
% 
% 
% Due to these problems we decided to go for a different strategy: cloning objects.
% Using the import module it is possible to ``teach'' the shared buffer how to clone any user-defined object by just providing a generic argument.
% A first library for the producer / consumer pattern thus consisted of the class \lstinline!CP_QUEUE! and \lstinline!CP_IMPORT_STRATEGY!, along with some predefined importers.
% 
% The import trick solves the main problem of the producer / consumer, namely migrating objects from producer to consumer efficiently.
% However, producers and consumers still have to deal with a nasty separate reference (the shared buffer), and there's also the problem that a user of the library might forget to import objects on the consumer side.
% 
% To overcome this problem we implemented a non-separate proxy class which automatically deals with the separate reference and imports.
% This idea proved to be so successful that eventually it was turned into its own pattern: the separate proxy.
% 
% \todo {Bon-style graphics of CP\_QUEUE and related items.}


\subsection{Process}

The process module provides a set of classes that all implement some sort of skeleton for a main loop.

The class \lstinline!CP_PROCESS! defines the interface.
It inherits from \lstinline!CP_STARTABLE!, such that clients have a simple way to start a separate process using \lstinline!CP_STARTABLE_UTILS!.

The feature \lstinline!start! is used to start the main loop.
As the process module defines the skeleton for a main loop, users are just required to implement \lstinline!step!, which should contain the body of the loop.
The loop can be exited by setting the attribute \lstinline!is_stopped! to \lstinline!True!.

\lstinline!CP_PROCESS! also introduces the two methods \lstinline!setup! and \lstinline!cleanup!.
They are called in the beginning or at the end of the main loop, and must be explicitly redefined by descendants if needed.

There are two different implementations for the main loop itself.
\lstinline!CP_CONTINUOUS_PROCESS! implements the main loop in a straightforward manner:
\begin{lstlisting}
from setup
until is_stopped
loop
  step
end
\end{lstlisting}
The advantage of this approach is its simplicity.
However, other processors never get a chance to access data which is handled by the processor of the \lstinline!CP_CONTINUOUS_PROCESS!, unless the main loop is exited completely.
This class is a simple implementation of the \lstinline!Active Object! pattern described in \todo{ref}.

The second implementation in \lstinline!CP_INTERMITTENT_PROCESS! is more interesting.
The basic idea is to perform only one iteration, and then tell another processor to inoke the loop body again in \lstinline!Current!.
This ping-pong approach ensures that after every iteration other processors may get a chance to access and modify data in \lstinline!CP_INTERMITTENT_PROCESS!.
In practice this is especially useful to stop a \lstinline!CP_INTERMITTENT_PROCESS! from the outside.

\lstinline!CP_INTERMITTENT_PROCESS! implements the Self Asynch pattern described in \todo{ref}.
The callback service is provided by the class \lstinline!CP_PACEMAKER!, and every \lstinline!CP_INTERMITTENT_PROCESS! automatically creates an associated pacemaker.

The \lstinline!CP_PERIODIC_PROCESS! is a refinement of the \lstinline!CP_INTERMITTENT_PROCESS!, which allows to add a small delay between executions.
It also adds a simple command \lstinline!stop!, which can be used to stop the process from the outside.
It is an implementation of the Timer: Periodic pattern in \todo{ref}.

\subsection{Worker pool}

% The worker pool module provides support classes to build a worker pool.
% It makes use of the queue module to store the work items.
% 
% The module consists of three classes:
% 
% \begin{itemize}
%  \item \lstinline!CP_WORKER!
%  \item \lstinline!CP_WORKER_POOL!
%  \item \lstinline!CP_WORKER_FACTORY!
% \end{itemize}
% 
% The last one is just an factory class to create the user-defined \lstinline!CP_WORKER! objects.
% 
% The deferred class \lstinline!CP_WORKER! has a predefined main loop, where the object first checks if it needs to terminate, then grabs a new work item, and processes it.
% The processing step is deferred and needs to be implemented by the user.
% 
% The \lstinline!CP_WORKER_POOL! is the central management instance.
% Its primary task is to accept new work items from clients, but it can also be used to adjust the number of workers in the pool and to terminate all workers.
% The \lstinline!CP_WORKER_POOL! also implements the separate proxy pattern, which means that clients should access it through \lstinline!CP_WORKER_POOL_PROXY!.
% 
% \subsection{Worker pool}

A worker pool is a set of threads that are ready to execute tasks.
The intention of the worker pool is to make use of parallelism but avoid the overhead of thread creation, which can be quite expensive especially for small tasks.

The main component of the worker pool is a shared buffer, where clients can insert tasks to be executed.
A worker thread will then repeatedly retrieve a task from the buffer and execute it.
The worker pool module makes use of the queue module, which provides the shared buffer.
  %producer / consumer pattern, with the library client as a producer and the worker threads as consumers.

%An important part of the worker pool is also the ability to increase or decrease the amount of worker threads, or to completely stop all worker threads such that the application can terminate.

The representation of a task to be executed varies between different languages.
In Java for instance a Runnable \todo{ref} object is used, whereas in C\# the task is represented as a delegate \todo{ref}.

In SCOOP there's the problem of object migration, as described in Section \ref{sec:object-migration}.
If the task object is created on its own processor, as in the Data Processor approach, you cancel out the performance gain you tried to achieve with the worker pool.
With the Lock Passing approach, a task object will be executed on the processor that created the object, which kind of makes the worker pool useless (not to mention the risks of processor starvation if applied wrong).
This only leaves the Import mechanism as a sensible solution.

In the library we support two flavors of a worker pool.
The first and more basic one is to have a deferred class \lstinline!CP_WORKER! where clients can directly implement an operation.
The object submitted to the worker pool then corresponds to the arguments of the operation.

The second solution uses a special task class to encapsulate an operation.
It is described in Section \ref{sec:arbitrary-operations}.

% However, there are two solutions to this problem: You can either use the import mechanism, or make the worker class deferred and let clients implement the task directly.
% In the library we support both approaches, although the latter can be used to implement the import solution.
% Section~\ref{sec:arbitrary-operations} shows how to do this.

The basic worker pool module has three main classes:
\begin{itemize}
 \item \lstinline!CP_WORKER_POOL!
 \item \lstinline!CP_WORKER!
 \item \lstinline!CP_WORKER_FACTORY!
\end{itemize}

The \lstinline!CP_WORKER_POOL! provides the shared buffer and some additional functionality to adjust the pool size.
The type of the task object alongside its import strategy can be specified with a generic argument.
\lstinline!CP_WORKER_POOL! inherits from \lstinline!CP_QUEUE! and therefore uses the same import mechanisms.

The deferred class \lstinline!CP_WORKER! corresponds to the worker thread in other languages.
Users need to implement the feature \lstinline!do_run!, which takes a task object and executes it.
The exact type of the task object depends on the generic arguments of \lstinline!CP_WORKER!, which should be the same as in \lstinline!CP_WORKER_POOL!.
The non-deferred part in \lstinline!CP_WORKER! is the main loop itself, which fetches a new task, calls \lstinline!do_run!, and checks if the worker needs to terminate.

The last class, \lstinline!CP_WORKER_FACTORY!, just provides a deferred factory function for a new worker.
This is necessary because the exact type of \lstinline!CP_WORKER! is not known to the library.
With a user-defined factory, the \lstinline!CP_WORKER_POOL! can create new workers on demand.




% \subsubsection{Terminating workers}

An important functionality of a worker pool is to adjust the number of workers.
Increasing the worker count is not a problem, as you can just create new \lstinline!CP_WORKER! instances using the factory.
To decrease the amount of workers, the module makes use of the processor termination technique described in Section \ref{sec:processor-termination}.


% However, decreasing the amount of workers is not that easy.
% 
% Java provides two builtin mechanisms to shut down a thread.
% You can either force it to stop, which immediately throws an exception in the thread \todo{ref}, or you can use the more collaborative interrupt mechanism \todo{ref}.
% The idea is that the thread will regularly check its interrupted flag and terminate on its own if requested.
% 
% The latter is also possible to do in SCOOP, except that there's no builtin interrupt flag.
% Instead a query \lstinline!is_stop_requested! in \lstinline!CP_WORKER_POOL! can be used.
% The main problem however are wait conditions.
% 
% In Java, blocking calls like \lstinline!wait()! and \lstinline!sleep()! may throw an \lstinline!InterruptedException! \todo{ref}.
% This avoids the problem that a thread may wait forever instead of shutting down, because all signaller threads have already terminated.
% Unfortunately, there's no such mechanism in SCOOP.
% It is possible however to work around this limitation by refining the wait condition:
% \begin{lstlisting}
% 
% class
%   CP_WORKER
%   
%   -- ...
%   
% feature -- Implementation
% 
%   fetch (pool: separate CP_WORKER_POOL)
%     require
%       not pool.is_empty or pool.is_stop_requested
%     do
%       if is_stop_requested then
% 	-- Stop the currrent worker.
%       else
% 	-- Grab the next item.
%       end
%     end
% 
% end
% \end{lstlisting}
% The additional \lstinline!if! statement is not very nice, but luckily it can be encapsulated completely in \lstinline!CP_WORKER!.
% 
% This code snippet is useful to break free of any wait condition if the requirements have changed.


The Separate Proxy pattern is applied to \lstinline!CP_WORKER_POOL! to simplify the use of a separate worker pool object.

The basic worker pool module allows for a very flexible use. 
It is for example possible to use it just as an advanced producer / consumer module where consumers are automatically created and destroyed.
The next section introduces a more advanced implementation of the worker pool which builds on the basic module.





\subsubsection{Arbitrary operations}
\label{sec:arbitrary-operations}

So far the task of a worker is defined in a user-defined \lstinline!CP_WORKER! class, and the object submitted to the worker pool mostly contains data.
The worker pool implementations in Java and C\# only accept Runnable (or delegate) objects.
This enables arbitrary operations that can be executed by the worker threads.

The SCOOP version of the worker pool can be enhanced to act like the Java / C\# counterparts.
To do that we need a class that represents an operation, and which can be moved across processor boundaries.

The agent classes in Eiffel (i.e. ROUTINE and descendants) may be used to represent operations, but they can't be easily imported.
That's why we added a new, deferred class \lstinline!CP_TASK!.
Users of the library can inherit from it and implement the feature \lstinline!run!.
\todo {Tell about agent integration?}

Using this interface it is possible to have a predefined \lstinline!CP_TASK_WORKER! that just runs \lstinline!CP_TASK! objects.
The associated \lstinline!CP_TASK_WORKER_POOL! implements the factory function and refines the raw \lstinline!CP_WORKER_POOL!.

The combination of these two classes is very close to the Java worker pool implementation.
The only difference is that a \lstinline!CP_TASK! object needs to be imported, whereas a Java Runnable object doesn't.


\subsection{Futures}

\subsubsection{Promise}

The promise module contains a set of classes which can be used to monitor the state of an asynchronous operation.

The main class is \lstinline!CP_PROMISE!, which defines several queries like \lstinline!is_terminated! or \lstinline!is_exceptional!.
It also defines the interface to cancel a task or to get the progress percentage (e.g. for a download task), but these queries need to be supported by the task itself.

The Separate Proxy mechanism is available for promise objects, because they are usually declared separate to the client.
In this case the pattern is implemented with four classes, i.e.
\begin{itemize}
 \item \lstinline!CP_PROMISE! defines a common interface,
 \item \lstinline!CP_SHARED_PROMISE! defines the actual separate object,
 \item \lstinline!CP_PROMISE_UTILS! defines helper functions to access a \lstinline!separate CP_PROMISE! and
 \item \lstinline!CP_PROMISE_PROXY! is the proxy object.
\end{itemize}

There's an important descendant, the \lstinline!CP_RESULT_PROMISE!, which is used for asynchronous operations that return a result.
It also has a set of associated classes that implement the Separate Proxy pattern.

The \lstinline!CP_RESULT_PROMISE! contains a query \lstinline!item! to retrieve the result as soon as it's available.
The return type of \lstinline!item! depends on a generic argument.
To migrate the result back to the client, this class makes use of the import module - i.e. the \lstinline!CP_SHARED_RESULT_PROMISE! and \lstinline!CP_RESULT_PROMISE_PROXY! both have an additional generic argument which defines the import strategy.

The \lstinline!CP_RESULT_PROMISE! is used for the future pattern.

\subsubsection{Executor}

The executor module can be used to execute varying operations.
The main class is \lstinline!CP_EXECUTOR!, which provides facilities to execute a \lstinline!CP_TASK! objects.

\lstinline!CP_TASK! itself represents a user-defined operation.
It can be imported across processor boundaries and provides exception handling.
To define a new task a client needs to inherit from \lstinline!CP_DEFAULT_TASK! and implement the feature \lstinline!run! and \lstinline!make_from_separate!.

A special kind of task is \lstinline!CP_COMPUTATION!, which can also return a value.
This is needed to implement the future pattern.

The \lstinline!CP_EXECUTOR! makes use of the separate proxy pattern.
However, the proxy also enhances the raw \lstinline!CP_EXECUTOR! interface with the option to attach a \lstinline!CP_BROKER! object to a task.
This can be used to query the status of a task, await termination, or get the computed result back in case of a \lstinline!CP_COMPUTATION!
All \lstinline!CP_BROKER! objects are created on a dedicated processor to avoid unnecessary thread creation.

An important implementation of the \lstinline!CP_EXECUTOR! is the \lstinline!CP_TASK_WORKER_POOL!.
As the name suggests, this is a worker pool where each worker repeatedly executes \lstinline!CP_TASK! objects.
\todo {More executor implementations?}

%\section {Individual patterns}
% Description of a few patterns




\subsubsection{Future}

The future pattern is used to perform a computation asynchronously.
Instead of computing a value directly, the computation gets wrapped into an object and the user only receives a handle to retrieve the value when it's ready.
This handle is often called Future, Promise or Delay.
In this thesis we'll use the term Future for the whole pattern, and Promise only refers to the handle.

The representation of the computation is a Callable object in Java and a delegate in C\#.
The Promise is usually a class with a generic argument that matches the result type of the computation.
A distinguishing feature of the Promise is that it blocks if the result is not yet available.

The execution of a computation object can vary.
In most cases the future pattern is backed by a worker pool, which executes the computation objects and updates the Promise with the correct result.

The main advantage of the future pattern is that it allows to make use of parallelism in an easy way.
A user just has to spot computations which may run asynchronously, and the future pattern takes care of thread management, synchronization and result propagation.

The implementation of the future pattern in SCOOP hits two problems:
\begin{itemize}
 \item Operations can't be easily moved from the client to an execution service.
 The same is also true for the result of a computation in the reverse direction.
 \item The promise object should neither be placed on the client processor nor on the executor service.
 The reason in both cases is that one processor may execute a main loop, which means the other processor never gets access to the promise object.
\end{itemize}

We'll use the standard procedure to deal with the first problem, namely the import mechanism.
The second problem is more interesting however.
One way to solve it is to create a third processor, which only takes care of the Promise object.
This ensures that both the library client and the execution service can access the Promise object, but it introduces a huge overhead.
For every computation one extra thread must be created!

A better tradeoff would be to introduce one global processor, which takes care of all promise objects.
This may introduce contention if multiple futures are submitted, but we think that this is acceptable.

However, this approach brings another problem.
A promise object has two generic arguments for the return type and the import strategy.
As these arguments are not known in advance, and because SCOOP processor tags \todo{ref} are not implemented yet, it is not possible to create a promise object on this dedicated processor.

The solution is - surprisingly - the import module.
We can create a promise object with the correct types on the client processor, and then ask the global processor to import it.
This way the promise object finally ends up on the correct processor.

In the library, the Promise object is provided by the class \lstinline!CP_PROMISE! and its descendants.
The computation is represented with \lstinline!CP_COMPUTATION!, or \lstinline!CP_TASK! for operations that don't return a result.
The execution service is the deferred class \lstinline!CP_EXECUTOR! and \lstinline!CP_TASK_WORKER_POOL! is the main implementation.

The separate proxy pattern is applied on \lstinline!CP_EXECUTOR! and \lstinline!CP_PROMISE!.
Besides acting as a processor-local proxy to a separate \lstinline!CP_EXECUTOR!, 
the classes \lstinline!CP_EXECUTOR_PROXY! and \lstinline!CP_FUTURE_EXECUTOR_PROXY! are also responsible to create Promise objects on the global processor.

\section{Evaluation and Benchmarks}


\todo{uncomment}%\input{tutorial}

\section{Conclusion}

\begin{appendix}
%\input{separate_proxy}
\end{appendix}

\todos


\end{document}          
