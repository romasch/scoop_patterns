\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

\usepackage{pattern}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{todo}

\newcommand{\dir} [1] [] {#1} 

% Title Page
\title{Concurrency Patterns in SCOOP}
\author{Roman Schmocker}


\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\section{Introduction}

\section {The SCOOP model}
% Introduction, differences to Java etc... (short)

\section {Pattern overview}
% Overview of all patterns, maybe tabular

\input{pattern_list}

\section {Library}

The library is designed as a set of modules which simplify concurrent programming in SCOOP.
Section ~\ref{sec:tutorial} explains how to use the library for commonly used patterns,
while Section ~\ref{sec:modules} concentrates on the design of the library itself.

\subsection {API tutorial}
\label{sec:tutorial}

\subsubsection{Producer / Consumer}

The producer / consumer example is pretty common in concurrent programming.
At its core is usually a shared buffer.
A producer can add items to the buffer, whereas a consumer removes items from the buffer.

The library class \lstinline!CP_QUEUE! can be used as a shared buffer.
If we want to use \lstinline!STRING! objects to be passed from producer to consumer, we have to declare the queue like this:

\begin{lstlisting}
class PRODUCER_CONSUMER feature

  make
      -- Launch producers and consumers.
    local
      queue: separate CP_QUEUE [STRING, CP_STRING_IMPORTER]
	-- ...
    do
      create queue.make_bounded (10)
	-- ...
    end
end
\end{lstlisting}

Note that there are two generic arguments:
\begin{itemize}
\item The first argument (\lstinline!STRING!) denotes the type of items in the queue.
\item The second argument (\lstinline!CP_STRING_IMPORTER!) defines the import strategy.
\end{itemize}

The import strategy is an important concept of the library.
An import in the SCOOP context means that an object on a separate processor should be copied to the local processor.
This is done recursively for any non-separate reference, i.e. for  \lstinline!STRING! you also have to copy the \lstinline!area! attribute.
The import strategy can be used to tell a component if a given object should be imported, and if yes, how it is done.

In our example we're using the \lstinline!CP_STRING_IMPORTER! to import strings.
An alternative would be to use \lstinline!CP_NO_IMPORTER [STRING]! if we want to disable imports.

The next step we need to do is to define the producer and consumer.

\lstinputlisting [firstline=7] {../../examples/producer_consumer/producer.e}

You may notice three things in this example:

\begin{itemize}
 \item \lstinline!PRODUCER! inherits from \lstinline!CP_STARTABLE!.
 \item The \lstinline!PRODUCER! uses a \lstinline!CP_QUEUE_PROXY! instead of the \lstinline!CP_QUEUE!.
 \item The generated strings are not separate.
\end{itemize}

The classes \lstinline!CP_STARTABLE! and \lstinline!CP_STARTABLE_UTILS! are a useful combination.
They allow to start some operation on a separate object without the need for a specialized wrapper function.

Another nice utility is the \lstinline!CP_QUEUE_PROXY!.
It is part of a pattern which is often used throughout the library - the separate proxy \todo{add reference}.
Basically it allows to access a separate queue without the need to deal with separate references.

The really interesting thing however is that the producer can generate strings on its local processor.
Usually this is not possible, because if the string is later passed to a consumer object, the latter needs to lock the producer in order to get access to the string.
But in this case we instructed the queue object to import all string objects. During a call to \lstinline!queue_wrapper.put(item)! the following happens:
\begin{itemize}
 \item The producer waits until it gets exclusive access to the queue.
 \item The separate call is executed. Because \lstinline!item! is a non-separate reference, the call is synchronous and lock passing happens.
 \item The queue object imports the separate string, creating a local copy.
 \item The separate call terminates, both processors can proceed autonomously.
\end{itemize}
Creating the strings on a separate processor is therefore unnecessary.

The import trick avoids a lot of unnecessary thread creation:
Instead of creating a new processor for every single produced item we just copy it, which is much faster for small objects.

The consumer is basically the same as the producer, except for the feature \lstinline!start!:

\begin{lstlisting}
class
  CONSUMER
--...
  
	start
			-- Consume `item_count' items.
		local
			i: INTEGER
			item: STRING
		do
			from
				i := 1
			until
				i > item_count
			loop
				queue_wrapper.consume

				check attached queue_wrapper.last_consumed_item as l_item then

						-- Note that `item' is not declared as separate, because it has been
						-- imported automatically.
					item := l_item
					print (item + " // Consumer " + identifier.out + ": item " + i.out + "%N")
				end
				i := i + 1
			end
		end
end
\end{lstlisting}

You might notice again that the consumed string is not declared as separate.
This is again because of the import mechanism within \lstinline!CP_QUEUE!.

The last thing we need to do is to create and launch the producers and consumers in the main application:

\lstinputlisting [firstline=7] {../../examples/producer_consumer/producer_consumer.e}


\subsubsection{Server thread}

In networking you often need a dedicated thread that listens on a socket.
In a SCOOP environment it is not hard to create such a processor, but it is hard to stop it.
The main problem is that the server processor will run its own main loop, and other threads ususally never get exclusive access to call some feature \lstinline!stop!.

The library addresses this issue with the \lstinline!CP_INTERMITTENT_PROCESS!.
This class defines a special main loop, where other processors can access the object after every iteration.

To use \lstinline!CP_INTERMITTENT_PROCESS! you need to inherit from it and implement the deferred feature \lstinline!step!.
The following example defines a simple echo server that just listens and replies with the same string:

\lstinputlisting [firstline=7] {../../examples/echo_server/echo_server.e}

To break out of the main loop, you need to set \lstinline!is_stopped! to \lstinline!True!.
This can be done from within the \lstinline!CP_INTERMITTENT_PROCESS! or from a separate processor by calling \lstinline!stop!.

To start the echo server you can use \lstinline!CP_STARTABLE_UTILS! and the feature \lstinline!asynch_start!:

\lstinputlisting [firstline=7] {../../examples/echo_server/echo_application.e}

\subsubsection{Futures}

A future is a computation which may run asynchronously, possibly returning a result.
The future is a very popular pattern in other languages like Java.

The idea is that the computation is encapsulated in an object which may be executed by another thread.
If there's a result to the computation, the client thread also gets a token back to access the result at some point in the future.
This token is usually called ``Future'' or ``Promise''.

The library mechanism to support futures are the class hierarchies rooted at \lstinline!CP_TASK!, \lstinline!CP_BROKER! and \lstinline!CP_EXECUTOR!.

The \lstinline!CP_DEFAULT_TASK! class is used to define the operation.
To use it you need to inherit from it and implement \lstinline!run! and \lstinline!make_from_separate!.
The latter is needed because SCOOP doesn't allow shared access to objects, which is why a \lstinline!CP_TASK! needs to be imported from one processor to another.
If the operation is returning a result, it is necessary to inherit from \lstinline!CP_COMPUTATION! and implement \lstinline!computed!.

\lstinline!CP_EXECUTOR! is used to submit a task to be executed.
The main implementation is \lstinline!CP_TASK_WORKER_POOL!, which is using a worker pool to execute \lstinline!CP_TASK! objects.
The executor shoud be accessed using a local \lstinline!CP_EXECUTOR_PROXY! and \lstinline!CP_FUTURE_EXECUTOR_PROXY!, which simplify access to the \lstinline!separate CP_EXECUTOR! and also initialize the Promise object.

The \lstinline!CP_BROKER! serves as the token to store the status of the computation and a possible result.
It is created on a separate processor, such that both the client and the task object can access it without the risk of a deadlock.
If you use \lstinline!CP_EXECUTOR_PROXY!, you can submit a task and receive a token object by using \lstinline!put_with_broker!.

\todo{integrate example}


\subsection {Architecture and modules}
\label{sec:modules}

The library consists of several modules, which often build on each other.

\subsection{Import}

The import module is one of the most important mechanisms in the library.
It consists of all classes in the directory \dir{library/import}.

The main class is the deferred \lstinline!CP_IMPORT_STRATEGY [G]!, which has the simple interface:


\lstinputlisting [firstline=7] {../../library/import/cp_import_strategy.e}

By implementing \lstinline!import!, descendants can decide if an object of type \lstinline!G! shall be imported and if yes, how it's done.

The class \lstinline!CP_NO_IMPORTER[G]! is the default class to disable import and just perform a reference copy.
Every other importer can inherit from \lstinline!CP_IMPORTER!, which narrows the return type of \lstinline!import! to a non-separate \lstinline!G!.

Because writing an extra importer for every importable object may be tedious, there's a support class \lstinline!CP_IMPORTABLE!:

\lstinputlisting [firstline=7] {../../library/import/cp_importable.e}

That way an import function can be written right inside the class that needs to be imported.
There are two main import classes that make use of it: \lstinline!CP_DYNAMIC_TYPE_IMPORTER! and \lstinline!CP_STATIC_TYPE_IMPORTER!.
The latter uses bounded genericity to directly create an object of type \lstinline!G!.
This has the drawback however that the type is statically \lstinline!G!, even if the argument to \lstinline!import! was of a subtype of \lstinline!G!.

The \lstinline!CP_DYNAMIC_TYPE_IMPORTER! tries to avoid this problem by using reflection.
This introduces a new problem with respect to void safety however, as the new object will not be initialized.
Therefore it is strongly advised to declare \lstinline!make_from_separate! as a creation procedure for every descendant of \lstinline!CP_IMPORTABLE!.

Another problem of the \lstinline!CP_DYNAMIC_TYPE_IMPORTER! are the invariants of an object.
There's a short time interval between the creation of an object (using reflection) and the call to \lstinline!{CP_IMPORTABLE}.make_from_separate! where the invariants are broken.
Due to this, it is not possible to have invariants in a class that will be imported using the dynamic type importer.

In the future, there will hopefully exist an import routine natively supported by the SCOOP runtime.
In that case \lstinline!CP_IMPORTER! can be made effective and use the native import, and all its descendants will become obsolete.


Other components of the library often use the import module via bounded genericity.
The class \lstinline!CP_QUEUE! for example has the ability to import objects, and it's class header looks like this:
\begin{lstlisting}
class
  [G, IMPORTER -> CP_IMPORT_STRATEGY [G] create default_create end]
feature
  ...
end
\end{lstlisting}
That way a user can decide on the precise semantics of the import strategy by just declaring the right type.

\subsection{Separate proxy}

The separate proxy pattern is a SCOOP specific mechanism.
The goal of the pattern is to provide a nice interface to a separate object, by providing a processor-local proxy which hides the separate reference.

Usually the pattern consists of three classes:
\begin{enumerate} [label=(\arabic*)]
 \item\label{item:sep-proxy:first} The class for the actual separate object.
 \item\label{item:sep-proxy:second} A class that provides helper functions to access a separate object of type \ref{item:sep-proxy:first}, usually with the ending \lstinline!_UTILS!.
 \item\label{item:sep-proxy:third} A proxy class with the a similar interface as \ref{item:sep-proxy:first}, usually ending on \lstinline!_PROXY!.
 Using \ref{item:sep-proxy:second}, the proxy forwards all calls to an object of type \ref{item:sep-proxy:first}.
\end{enumerate}

\todo{A little diagram showing the class relations.}

It is possible to add a fourth, deferred class that just defines the interface for \ref{item:sep-proxy:first} and \ref{item:sep-proxy:third}.
However, there's an inconsistency: 
Any precondition in \ref{item:sep-proxy:first} which references \lstinline!Current! needs to be converted to a wait condition in \ref{item:sep-proxy:third} that references \ref{item:sep-proxy:first}.
Furthermore, not all features in the business class may be necessary in the proxy, and the proxy itself may add some more features such as compound actions.

Unfortunately this pattern cannot be fully turned into a module, because it's highly dependent on the precise interface of the business class.
There is however some support in the library: 
\lstinline!CP_PROXY! defines the creation procedure and the attributes \lstinline!subject! for a business class object and \lstinline!utils! for a helper object.

The following example shows a general recipe on how to create a separate proxy for the small class \lstinline!EXAMPLE!:

\begin{lstlisting}
deferred class
  EXAMPLE [G]

feature -- Status report

  is_available: BOOLEAN
      -- Is `item' available?
  
feature -- Access

  item: separate G
      -- Item in `Current'.
    require
      available: is_available
    deferred
    end

feature -- Element change

  put (a_item: separate G)
      -- Set `item' to `a_item'.
    deferred
    end

end
\end{lstlisting}

First we need to create the helper class.
This is done according to these rules:
 \begin{itemize}
  \item The name should be \lstinline!EXAMPLE_UTILS!.
  \item The generic arguments are the same as in \lstinline!EXAMPLE!
  \item Any feature to access the separate \lstinline!EXAMPLE! object can be prefixed by \lstinline!example_!.
  This helps to avoid name clashes if someone wants to inherit from \lstinline!EXAMPLE_UTILS!
  \item The first argument of each feature is \lstinline!example: separate EXAMPLE [G]!.
  All other arguments are the same as the ones in the corresponding feature in \lstinline!EXAMPLE!.
  \item Preconditions in \lstinline!EXAMPLE! should be rewritten as wait conditions with the same meaning in \lstinline!EXAMPLE_UTILS!.
  \item If there's a non-expanded return type to a feature, you can decide if it should be declared separate in \lstinline!EXAMPLE_UTILS! or if it should be imported.
 \end{itemize}

\begin{lstlisting}
class
  EXAMPLE_UTILS [G]
  
feature -- Access

  example_item (example: separate EXAMPLE [G]): separate G
      -- Get the item from `example'.
      -- May block if not yet available.
    require
      available: example.is_available
    do
      Result := example.item
    end

feature -- Element change
 
  example_put (example: separate EXAMPLE [G]; item: separate G)
      -- Put `item' into `example'.
    do
      example.put (item)
    end
end
\end{lstlisting}

In this example we also dropped the feature \lstinline!is_available!, because it's not considered to be important for separate clients.

The proxy class has also has some simple rules:

 \begin{itemize}
  \item The name should be \lstinline!EXAMPLE_PROXY!.
  \item The generic arguments are the same as in \lstinline!EXAMPLE!
  \item The class can inherit from \lstinline!CP_PROXY [EXAMPLE [G], EXAMPLE_UTILS [G]]! and declare \lstinline!make! as a creation procedure.
  \item The feature names and arguments are the same as in \lstinline!EXAMPLE!.
  \item Preconditions in \lstinline!EXAMPLE! are usually not present in \lstinline!EXAMPLE_PROXY!. They are wait conditions in \lstinline!EXAMPLE_UTILS! instead.
  \item Every feature implementation makes use of \lstinline!utils! to forward the request to the \lstinline!subject!.
 \end{itemize}

\begin{lstlisting}
class
  EXAMPLE_PROXY [G]

inherit
  CP_PROXY [EXAMPLE [G], EXAMPLE_UTILS [G]]

create
  make
  
feature -- Access

  item: separate G
      -- Item in the example object.
      -- May block if not yet available.
    do
      Result := utils.cell_item (subject)
    end

feature -- Element change

  put (a_item: separate G)
      -- Set `item' to `a_item'.
    do
      utils.cell_put (subject, a_item)
    end

end
\end{lstlisting}
 
\subsection{Queue}

The queue module is a simple queue implementation.
It uses the separate proxy pattern, which means that it consists of three classes:

\begin{itemize}
 \item \lstinline!CP_QUEUE!
 \item \lstinline!CP_QUEUE_UTILS!
 \item \lstinline!CP_QUEUE_PROXY!
\end{itemize}

The first one provides the actual queue, but internally it just relies on \lstinline!ARRAYED_QUEUE!.
The class \lstinline!CP_QUEUE_PROXY! can be used to access such a shared, separate queue without having to deal with separate references.

The interesting thing about this queue implementation is that it makes use of the import module.
Along with the generic argument \lstinline!G! you can also provide an \lstinline!CP_IMPORT_STRATEGY [G]!.
The import then happens automatically in both the queue and its proxy.

\subsection{Process}

The process module provides a set of classes that all implement some sort of skeleton for a main loop.

\todo{more text}

\subsection{Worker pool}

The worker pool module provides support classes to build a worker pool.
It makes use of the queue module to store the work items.

The module consists of three classes:

\begin{itemize}
 \item \lstinline!CP_WORKER!
 \item \lstinline!CP_WORKER_POOL!
 \item \lstinline!CP_WORKER_FACTORY!
\end{itemize}

The last one is just an factory class to create the user-defined \lstinline!CP_WORKER! objects.

The deferred class \lstinline!CP_WORKER! has a predefined main loop, where the object first checks if it needs to terminate, then grabs a new work item, and processes it.
The processing step is deferred and needs to be implemented by the user.
Implemented

The \lstinline!CP_WORKER_POOL! is the central management instance.
Its primary task is to accept new work items from clients, but it can also be used to adjust the number of workers in the pool and to terminate all workers.
The \lstinline!CP_WORKER_POOL! also implements the separate proxy pattern, which means that clients should access it through \lstinline!CP_WORKER_POOL_PROXY!.

\subsection{Promise}

The promise cluster contains a set of classes which can be used to monitor the state of an asynchronous operation.

\todo{}

\subsection{Executor}

The executor module can be used to execute varying operations.
The main class is \lstinline!CP_EXECUTOR!, which provides facilities to execute a \lstinline!CP_TASK! objects.

\lstinline!CP_TASK! itself represents a user-defined operation.
It can be imported across processor boundaries and provides exception handling.
To define a new task a client needs to inherit from \lstinline!CP_DEFAULT_TASK! and implement the feature \lstinline!run! and \lstinline!make_from_separate!.

A special kind of task is \lstinline!CP_COMPUTATION!, which can also return a value.
This is needed to implement the future pattern.

The \lstinline!CP_EXECUTOR! makes use of the separate proxy pattern.
However, the proxy also enhances the raw \lstinline!CP_EXECUTOR! interface with the option to attach a \lstinline!CP_BROKER! object to a task.
This can be used to query the status of a task, await termination, or get the computed result back in case of a \lstinline!CP_COMPUTATION!
All \lstinline!CP_BROKER! objects are created on a dedicated processor to avoid unnecessary thread creation.

An important implementation of the \lstinline!CP_EXECUTOR! is the \lstinline!CP_TASK_WORKER_POOL!.
As the name suggests, this is a worker pool where each worker repeatedly executes \lstinline!CP_TASK! objects.
\todo {More executor implementations?}

\section {Individual patterns}
% Description of a few patterns

\subsection{Producer / Consumer}

The producer / consumer is a very popular pattern in concurrent programming, and it is a building block for other patterns like pipeline as well.
The basic idea is to have a shared, concurrent buffer.
Producer threads put new items into the buffer, whereas consumer threads remove items from this buffer.

\todo{BON-style diagram and ``migration graphics''}

In threaded systems, this buffer is usually accessed by several threads at the same time.
Careful synchronization has to be enforced to ensure that the buffer remains in a consistent state.
The data items on the other hand ``migrate'' from a producer to the buffer, and then to exactly one consumer.
They therefore don't need to be thread-safe as long as the producer promises never to touch the item again.

In SCOOP things look a bit different however.
Due to the exclusive access guarantee it is not necessary to establish a synchronization policy.
The downside however is a loss of potential concurrency when a producer and a consumer accesses the buffer simultaneously, but this is a minor problem.

The main problem in SCOOP are the data items, especially if they are not of an expanded type.
If the object is created on the producer processor, then the consumer needs to control the producer in order to get access to the object.
This is clearly a situation that we want to avoid, because it couples the producer and consumer in a vicious hidden way, and the whole point of the producer / consumer pattern is to decouple the two.

A nice solution would be if it's somehow possible to migrate data items, like it's done in threaded languages.
However, this is not possible with the current semantics of SCOOP, because an object always stays on the processor it was created on.

One approach to solve this problem is to create a new processor for every data item.
This actually works, but it can be very slow, especially for a lot of small data items.
There are two reasons for this:
First, every SCOOP processor is mapped to an operating system thread, therefore creating a new processor involves creating a new thread which is an expensive operation.
The second reason is the overhead of separate calls itself.
This has to be paid every time the consumer wants to access the separate object.

Another problem of this approach is related to ease of programming.
Dealing with a separate object can be very annoying, because you need to write small wrapper functions for every little feature call.

\begin{lstlisting}
class
  CONSUMER

feature -- Basic operations
  
  retrieve
      -- Retrieve a string and print its length.
    local
      l_string: separate STRING
    do
      l_string := buffer_consume (a_queue)
      print (string_count (l_string))
    end
    
feature {NONE} -- Implementation
  
  buffer: separate BUFFER [STRING]

  buffer_consume (a_buffer: like buffer): separate STRING
      -- An annoying small wrapper function for a buffer.
    do
      Result := a_buffer.item
      a_buffer.remove
    end
    
  string_count (a_string: separate STRING): INTEGER
      -- An annoying small wrapper function for a string.
    do
      Result := a_string.count
    end
end
\end{lstlisting}


Due to these problems we decided to go for a different strategy: cloning objects.
Using the import module it is possible to ``teach'' the shared buffer how to clone any user-defined object by just providing a generic argument.
A first library for the producer / consumer pattern thus consisted of the class \lstinline!CP_QUEUE! and \lstinline!CP_IMPORT_STRATEGY!, along with some predefined importers.

The import trick solves the main problem of the producer / consumer, namely migrating objects from producer to consumer efficiently.
However, producers and consumers still have to deal with a nasty separate reference (the shared buffer), and there's also the problem that a user of the library might forget to import objects on the consumer side.

To overcome this problem we implemented a non-separate proxy class which automatically deals with the separate reference and imports.
This idea proved to be so successful that eventually it was turned into its own pattern: the separate proxy.

\todo {Bon-style graphics of CP\_QUEUE and related items.}

\subsection{Worker pool}

A worker pool is a set of threads that are ready to execute tasks.
The intention of the worker pool is to make use of parallelism but avoid the overhead of thread creation, which can be quite expensive especially for small tasks.

The main component of the worker pool is a shared buffer, where clients can insert tasks to be executed.
A worker thread will then repeatedly retrieve a task from the buffer and execute it.
Thus the worker pool pattern makes itself use of the producer / consumer pattern, with the library client as a producer and the worker threads as consumers.

An important part of the worker pool is also the ability to increase or decrease the amount of worker threads, or to completely stop all worker threads such that the application can terminate.

The representation of a task to be executed varies between different languages.
In Java for instance a Runnable \todo{ref} object is used, whereas in C\# the task is represented as a delegate \todo{ref}.

In SCOOP there's the usual problem that an object can't be migrated.
This is especially bad for objects that encapsulate a task to be executed, as the task will be executed on the processor that created the object.
This fact seems to make worker pools in SCOOP effectively useless.
If the task object is created on its own processor, you cancel out the performance gain you tried to achieve with the worker pool.
And if the task object is created on the client processor, you may introduce deadlocks, or in the best case just a straightforward sequential execution.

However, there are two solutions to this problem: You can either use the import mechanism, or make the worker class deferred and let clients implement the task directly.
In the library we support both approaches, although the latter can be used to implement the import solution.
Section~\ref{sec:arbitrary-operations} shows how to do this.

The basic worker pool module therefore has three main classes:
\begin{itemize}
 \item \lstinline!CP_WORKER_POOL!
 \item \lstinline!CP_WORKER!
 \item \lstinline!CP_WORKER_FACTORY!
\end{itemize}

The \lstinline!CP_WORKER_POOL! provides the shared buffer and some additional functionality to adjust the pool size.
The type of the task object alongside its import strategy can be specified with a generic argument.
This allows a very flexible use of the worker pool, it is for example possible to use it just as an advanced producer / consumer module where consumers are automatically created and destroyed.

The deferred class \lstinline!CP_WORKER! corresponds to the worker thread in other languages.
Users need to implement the feature \lstinline!do_run!, which takes a task object and executes it.
The exact type of the task object depends on the generic arguments of \lstinline!CP_WORKER!, which should be the same as in \lstinline!CP_WORKER_POOL!.
The non-deferred part in \lstinline!CP_WORKER! is the main loop itself, which fetches a new task, calls \lstinline!do_run!, and checks if the worker needs to terminate.

The last class, \lstinline!CP_WORKER_FACTORY!, just provides a deferred factory function for a new worker.
This is necessary because the exact type of \lstinline!CP_WORKER! is not known to the library.
With a user-defined factory, the \lstinline!CP_WORKER_POOL! can create new workers on demand.

To simplify the handling of a worker pool we also applied the Separate Proxy pattern on \lstinline!CP_WORKER_POOL!.

\subsubsection{Terminating workers}

An important functionality of a worker pool is to adjust the number of workers.
Increasing the worker count is not a problem, as you can just create new \lstinline!CP_WORKER! instances using the factory.
However, decreasing the amount of workers is not that easy.

Java provides two builtin mechanisms to shut down a thread.
You can either force it to stop, which immediately throws an exception in the thread \todo{ref}, or you can use the more collaborative interrupt mechanism \todo{ref}.
The idea is that the thread will regularly check its interrupted flag and terminate on its own if requested.

The latter is also possible to do in SCOOP, except that there's no builtin interrupt flag.
Instead a query \lstinline!is_stop_requested! in \lstinline!CP_WORKER_POOL! can be used.
The main problem however are wait conditions.

In Java, blocking calls like \lstinline!wait()! and \lstinline!sleep()! may throw an \lstinline!InterruptedException! \todo{ref}.
This avoids the problem that a thread may wait forever instead of shutting down, because all signaller threads have already terminated.
Unfortunately, there's no such mechanism in SCOOP.
It is possible however to work around this limitation by refining the wait condition:
\begin{lstlisting}

class
  CP_WORKER
  
  -- ...
  
feature -- Implementation

  fetch (pool: separate CP_WORKER_POOL)
    require
      not pool.is_empty or pool.is_stop_requested
    do
      if is_stop_requested then
	-- Stop the currrent worker.
      else
	-- Grab the next item.
      end
    end

end
\end{lstlisting}
The additional \lstinline!if! statement is not very nice, but luckily it can be encapsulated completely in \lstinline!CP_WORKER!.

This code snippet is useful to break free of any wait condition if the requirements have changed.

\subsubsection{Arbitrary operations}
\label{sec:arbitrary-operations}

So far the task of a worker is defined in a user-defined \lstinline!CP_WORKER! class, and the object submitted to the worker pool mostly contains data.
The worker pool implementations in Java and C\# only accept Runnable (or delegate) objects.
This enables arbitrary operations that can be executed by the worker threads.

The SCOOP version of the worker pool can be enhanced to act like the Java / C\# counterparts.
To do that we need a class that represents an operation, and which can be moved across processor boundaries.

The agent classes in Eiffel (i.e. ROUTINE and descendants) may be used to represent operations, but they can't be easily imported.
That's why we added a new, deferred class \lstinline!CP_TASK!.
Users of the library can inherit from it and implement the feature \lstinline!run!.
\todo {Tell about agent integration?}

Using this interface it is possible to have a predefined \lstinline!CP_TASK_WORKER! that just runs \lstinline!CP_TASK! objects.
The associated \lstinline!CP_TASK_WORKER_POOL! implements the factory function and refines the raw \lstinline!CP_WORKER_POOL!.

The combination of these two classes is very close to the Java worker pool implementation.
The only difference is that a \lstinline!CP_TASK! object needs to be imported, whereas a Java Runnable object doesn't.

\subsection{Future}

The future pattern is used to perform a computation asynchronously.
Instead of computing a value directly, the computation gets wrapped into an object and the user only receives a handle to retrieve the value when it's ready.
This handle is often called Future, Promise or Delay.
In this thesis we'll use the term Future for the whole pattern, and Promise only refers to the handle.

The representation of the computation is a Callable object in Java and a delegate in C\#.
The Promise is usually a class with a generic argument that matches the result type of the computation.
A distinguishing feature of the Promise is that it blocks if the result is not yet available.

The execution of a computation object can vary.
In most cases the future pattern is backed by a worker pool, which executes the computation objects and updates the Promise with the correct result.

The main advantage of the future pattern is that it allows to make use of parallelism in an easy way.
A user just has to spot computations which may run asynchronously, and the future pattern takes care of thread management, synchronization and result propagation.

The implementation of the future pattern in SCOOP hits two problems:
\begin{itemize}
 \item Operations can't be easily moved from the client to an execution service.
 The same is also true for the result of a computation in the reverse direction.
 \item The promise object should neither be placed on the client processor nor on the executor service.
 The reason in both cases is that one processor may execute a main loop, which means the other processor never gets access to the promise object.
\end{itemize}

We'll use the standard procedure to deal with the first problem, namely the import mechanism.
The second problem is more interesting however.
One way to solve it is to create a third processor, which only takes care of the Promise object.
This ensures that both the library client and the execution service can access the Promise object, but it introduces a huge overhead.
For every computation one extra thread must be created!

A better tradeoff would be to introduce one global processor, which takes care of all promise objects.
This may introduce contention if multiple futures are submitted, but we think that this is acceptable.

However, this approach brings another problem.
A promise object has two generic arguments for the return type and the import strategy.
As these arguments are not known in advance, and because SCOOP processor tags \todo{ref} are not implemented yet, it is not possible to create a promise object on this dedicated processor.

The solution is - surprisingly - the import module.
We can create a promise object with the correct types on the client processor, and then ask the global processor to import it.
This way the promise object finally ends up on the correct processor.

In the library, the Promise object is provided by the class \lstinline!CP_PROMISE! and its descendants.
The computation is represented with \lstinline!CP_COMPUTATION!, or \lstinline!CP_TASK! for operations that don't return a result.
The execution service is the deferred class \lstinline!CP_EXECUTOR! and \lstinline!CP_TASK_WORKER_POOL! is the main implementation.

The separate proxy pattern is applied on \lstinline!CP_EXECUTOR! and \lstinline!CP_PROMISE!.
Besides acting as a processor-local proxy to a separate \lstinline!CP_EXECUTOR!, 
the classes \lstinline!CP_EXECUTOR_PROXY! and \lstinline!CP_FUTURE_EXECUTOR_PROXY! are also responsible to create Promise objects on the global processor.

\subsection{Evaluation and Benchmarks}


\section{Conclusion}

\todos
\end{document}          
