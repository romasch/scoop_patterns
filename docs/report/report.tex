\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{pattern}
\usepackage{todo}

% Nice fonts
\usepackage{palatino}
% Needed for Listings package with Eiffel.
\usepackage{xcolor}
% Source code listings.
\usepackage{listings}
% Appendix with extra title.
\usepackage [toc, page] {appendix}
% To include PNG files.
\usepackage{graphicx}
% Nice looking captions.
\usepackage[font={footnotesize,sl}, labelfont=bf] {caption}

% Clickable links. Has to be the last package:
\usepackage [hidelinks] {hyperref}


\lstset{language=OOSC2Eiffel,basicstyle=\ttfamily\small}
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\setlength{\headheight}{28pt}
\lstset{escapechar=\$}

% \newcommand{\dir} [1] [] {\emph{#1}} 
\newcommand{\dir}{\emph}
\newcommand{\todoref}{\todo{ref}}

% Title Page
\title{Concurrency Patterns in SCOOP}
\author{Roman Schmocker}


\begin{document}
\maketitle

\begin{abstract}
The wide distribution of multi-core processors increasingly forces programmers to deal with concurrency.
Parallel programming is not easy, but it can be simplified with the use of well-known patterns.

We have investigated which patterns are used in practice and compiled an exhaustive list with pattern descriptions.
Some of these patterns were then implemented as a reusable library for Eiffel and SCOOP.
We also describe some of the challenges when programming in SCOOP for the first time and provide solutions.
\end{abstract}


\tableofcontents

\section{Introduction}
\label{sec:introduction}

Due to the advent of multicore processors, concurrent programming has become an important part in software engineering.
Dealing with parallelism isn't easy however.
There are many pitfalls, such as race conditions and deadlocks.

In practice programmers have learned to avoid tricky concurrency problems with the use of some well-known patterns.
These patterns are often shipped with the standard library of the language, such that users rarely have to implement them.

The Eiffel programming language \cite{web:ecma-eiffel}\cite{book:touchofclass} has a new concurrency extension called SCOOP \cite{Nienaltowski07}\cite{web:scoop},
which stands for Simple Concurrent Object-Oriented Programming.
SCOOP simplifies concurrent programming a lot and eliminates one source of errors completely, namely race conditions \cite{Nienaltowski07}.
However, there is little experience on how to implement popular concurrency patterns, such as a worker pool, in SCOOP.

This thesis tries to fill this gap by providing a library of reusable concurrency patterns as well as methodical advice on programming in SCOOP.
The main contributions of the thesis are:
\begin{itemize}
 \item A broad survey of known concurrency patterns.
 \item The identification of common SCOOP challenges and advice on how to solve them.
 \item A new concurrency library which provides implementations for some selected patterns.
 This selection was mainly based on input from the ``Concurrency made easy'' team at ETH Zürich and the study of the Java \cite{web:java-concurrency} and C\# \cite{web:ms-tpl} concurrency libraries.
\end{itemize}

\subsection{Overview}

Section \ref{sec:pattern_overview} introduces a list of concurrency patterns which we found and categorized by studying literature and the standard libraries.
A brief introduction to the SCOOP model is given in Section \ref{sec:scoop-model}.
Section \ref{sec:scoop-challenges} describes some challenges when programming in SCOOP and how to solve them.
The latter two sections may be interesting for programmers with experience in threaded programming, but who wish to learn SCOOP.

The focus of Section \ref{sec:library} is on the goals and concepts of the concurrency patterns library.
It also provides a brief overview over the available modules, and which patterns are implemented by which modules.

A detailed explanation over the individual modules is provided by Section \ref{sec:modules}.
Finally, Section \ref{sec:evaluation} provides a small performance evaluation of the library.

\section {Pattern overview}
\label{sec:pattern_overview}
% Overview of all patterns, maybe tabular

\input{pattern_list}

\section {The SCOOP model}
\label {sec:scoop-model}
% Introduction, differences to Java etc... (short)

SCOOP is an extension to the Eiffel programming language that aims to make concurrent programming easier.
The basic idea is that every object can only be accessed by exactly one computational unit.
This unit is called processor, or handler of an object.

The keyword \lstinline!separate! is used to indicate that an object may be handled by a processor with respect to the handler for \lstinline!Current!.
Calls to a separate object (``separate calls'') then correspond to sending a message to the foreign processor.
There are two types of separate calls: synchronous and asynchronous.
If the called feature returns a result, the call is synchronous, which means that the current processor has to wait for the foreign processor to finish its task.
An asynchonous call happens when the feature is a command, i.e. not returning any result.
In that case, both processors can proceed concurrently.

A separate call is only allowed if the target of the call is ``controlled''.
Having an object controlled means that the user has exclusive access to that object - in that sense controlling an object corresponds a bit to locking in other languages.
In order to control an object it has to appear as a formal argument in the enclosing routine.

SCOOP guarantees that all messages sent by the current processor are handled in the foreign processor in the correct order.
This, along with the exclusive access guarantee, ensures that a controlled separate object behaves just like an object in a sequential program.
This is the reason why the SCOOP model is so simple: 
It allows reasoning about a feature body without the need to consider all possible interleavings of two parallel executions.

A new processor is created by calling a creation instruction on an object which is declared as separate.
The new object is then handled by the new processor.

Preconditions in SCOOP have a special role.
In a concurrent setting there's often the problem that a correctness condition may change due to unfortunate interleaving, 
e.g. between checking that a buffer is not empty and then removing an item, the buffer actually becomes empty due to interference from another thread.
Therefore SCOOP turns preconditions into wait conditions if they reference a separate object.

There are many advantages to the SCOOP model, such as easier reasoning and absence of data races, but it also has some shortcomings.
It is, for example, often necessary to write lots of little helper functions that just take a separate object and perform a single call on it.
This is due to the fact that every call to a separate object must be controlled.

SCOOP also has performance problems, because it packs every separate call into a message to another processor.
This is rather expensve, especially for small functions like array access.
Additionally, a SCOOP processor is currently implemented as an operating system thread, and creating them is an costly operation that involves context switches.
The SCOOP model however encourages the creation of many processors, which is not ideal for performance reasons.

\section{Challenges in SCOOP}
\label{sec:scoop-challenges}
% This section describes recurring challenges in SCOOP and how to solve them...

\subsection{Object migration}
\label{sec:object-migration}

Passing data from one processor to another is often necessary when programming in SCOOP.
The most obvious example is the producer/consumer pattern, but it also applies to other situations where one just wants to provide some arguments to an asynchronous command.

There are two types of objects one can pass as arguments.
The first type are expanded objects, which also includes basic types such as \lstinline!INTEGER!.
Passing expanded objects from one processor to another is not a problem in SCOOP due to their copy-semantics property.
However, passing reference objects is a bit more tricky.
Bad things such as processor starvation or unintentional lock passing may happen if done wrong.

There are basically three ways to safely pass reference objects from a sender to a receiver processor.
The first and easiest solution is to create data on its own, separate processor: 
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Migrate objects on a separate processor.}]
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: separate ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This approach is conceptually easy, but it's not very efficient, especially when the argument object is small.
We'll call this solution the Data Processor approach.

Another solution is to create the object on the same handler as the sender object:
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Migrate objects with lock passing.}]
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.do_something (args)
    end
end

class RECEIVER feature 
  do_something (args: separate ANY)
      -- Perform some operation with `args'.
    do
      print (args)
    end
end
\end{lstlisting}
This solution (the Lock Passing approach) looks almost like the first one.
The only change is a missing separate keyword.
However, it's semantics are radically different:

\begin{itemize}
 \item The feature \lstinline!do_something! is executed synchronously due to the lock passing mechanism \cite[p. 152]{Nienaltowski07}\cite{web:scoop}.
 This means that the sender class needs to wait for it to finish.
 \item \lstinline!RECEIVER! can't lock the argument object any more after \lstinline!do_something! finishes.
 In particular this means that the receiver class should not store the argument in one of its attributes, because any attempt to access it later will likely result in starvation.
 The reason for this is that the handler of the sender object will continue its execution, and as long as there's still work to do no other processor can access objects on it .
 \item Compared to the first approach, no new processor is created.
\end{itemize}

The last method makes use of a special SCOOP function called \lstinline!import!:
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Migrate objects with import.}]
class SENDER feature
  send (a_receiver: separate RECEIVER)
      -- Invoke an asynchronous operation with
      -- an argument on `a_receiver'.
    local
      args: ANY
    do
      create args
      a_receiver.receive_args (args)
      a_receiver.do_something
    end
end

class RECEIVER feature
  
  received: ANY
  
  receive_args (args: separate ANY)
      -- Receive some arguments
    do
      received := import (args)
    end

  do_something
      -- Perform some operation.
    do
      print (received)
    end
end
\end{lstlisting}
The \lstinline!import! feature copies its argument object, along with all non-separate references, to the local processor.
It is somewhat similar to \lstinline!{ANY}.deep_copy!, except that it doesn't clone separate references.

This import solution has several advantages.
There's no need for a new processor, and the receiver can keep the data and do the operation asynchronously.
The drawback is that the data needs to be copied.
However, for small data items this is actually faster than creating a new thread.

Note that \lstinline!receive_args! is executed synchronously just like in the Lock Passing approach.
Therefore, to execute \lstinline!do_something! asynchronously, it has to be divided into an execution and argument receiving part.

The feature \lstinline!import! was first described in \cite[p. 106]{Nienaltowski07}, but unfortunately it is not implemented in current SCOOP.
It is possible however to implement it manually with some user support.

\subsection{Processor communication}
\label{sec:processor-communication}
% Problem that two concurrent, active processors can't communicate. Example downloader task.
% Solution: a third, passive processor with a shared data structure.

It is often necessary that two threads need to communicate with each other.
One example would be a user interface with a background downloader task.
The user interface needs to be able to cancel the downloader, and the downloader needs to inform the GUI that it is finished.

In SCOOP this is not easily done.
Both processors are performing a long-running execution, which doesn't allow other processors to do separate calls on them.
Specifically, the GUI processor is in a main loop to receive input and repaint the window, whereas the downloader task is busy receiving chunks of data.
Cancellation will not work in this case, because the user interface processor will have to wait for the download processor to finish until it can actually access the downloader task to call \lstinline!cancel!, 
which kind of defeats the purpose of the cancellation button.
Even worse, the user interface will freeze until the GUI processor breaks free of its wait condition.

The solution to this kind of problem is to introduce a third processor which is ``passive'', meaning that it doesn't have a task to perform and only waits for incoming requests.
This third processor is known to the other two, ``active'' processors, and it contains all the attributes which are necessary for communication.
In our example this means that the ``passive'' processor takes care of an object with an \lstinline!is_cancelled! and \lstinline!is_finished! boolean flag.
The ``active'' processors then regularly need to check the status of these flags.

The solution to the task cancellation problem comes from this paper \cite{paper:task-cancellation}.

\subsection{Processor termination}
\label{sec:processor-termination}
% Problem: how to stop consumer waiting on empty buffer.
% Solution: Query is_stopped in shared buffer.

When an application needs to be shut down, it is necessary to stop any running threads.
Sometimes this can be done via the ``passive'' third processor as seen in Section \ref{sec:processor-communication}.
A problem arises however when the active processor is stuck in a wait condition.

One example of this could be the producer/consumer pattern, where a consumer is waiting for a buffer to become non-empty.
If all producers are already terminated, the consumer never gets the chance to break out of this wait condition and therefore cannot terminate successfully.

The solution is to add a query \lstinline!is_stop_requested! in the shared buffer, and to adapt the wait condition to include the stop request:

\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={Breaking out of a wait condition.}]
class
  CONSUMER

feature -- Status report

  buffer: separate BUFFER
  
  last_item: INTEGER
  
  is_stopped: BOOLEAN
  
feature -- Basic operations
  
   start
      -- Start the main loop
    do
      from 
      until 
	is_stopped
      loop
	fetch (buffer)
	if not is_stopped then
	  -- do something
	end
      end
    end
  
feature -- Implementation

  fetch (buf: separate BUFFER)
      -- Get the next item from `buf'.
    require
      not buf.is_empty or buf.is_stop_requested
    do
      if buf.is_stop_requested then
	is_stopped := True
      else
	last_item := buf.item
	buf.remove
      end
    end
end
\end{lstlisting}

This allows a buffer to exit the wait condition even when the buffer is empty.
The drawback of this approach is that it clutters the application code with some additional if-else constructs, 
but it is often possible to hide them in a special \lstinline!fetch! function as shown in our example.

\section {Library}
\label{sec:library}

% The library is designed as a set of modules which simplify concurrent programming in SCOOP.
% Section ~\ref{sec:tutorial} explains how to use the library for commonly used patterns,
% while Section ~\ref{sec:modules} concentrates on the design of the library itself.
 
\subsection{Goals}
\label{sec:goals}

The goal of the library is to provide a set of tools that simplify programming in SCOOP.
Specifically, we want to provide implementations for common concurrency patterns like the worker pool.
The result should be a new SCOOP library similar to the standard concurrency libraries in Java \cite{web:java-concurrency} or C\# \cite{web:ms-tpl}.

The library was developed with the following design goals:

\begin{description}
 \item [Safety]\label{item:safety} Avoid common SCOOP pitfalls, like deadlock problems, starvation of a processor, or unintentional lock passing.
 \item [Convenience]\label{item:convenience} Shield the user from having to write ``small wrapper'' features, i.e. features that need to be written just to lock an object for a separate call.
 \item [Performance]\label{item:performance} Reduce the overhead of thread creation, especially for concurrent programs that involve dealing with a lot of small separate objects.
\end{description}

\subsection{Concepts}

This section describes the core concepts of the library: Import and Separate Proxies.
The import concept deals with the problem of how to pass data from one processor to another.
It is usefol to achieve the performance goal and to some extent the safety goal in Section \ref{sec:goals}.

The Separate Proxy \patternref{SP} is a pattern to hide separate references behind a proxy object.
It provides a solution to the convenience design goal.

\subsubsection{Import}
\label{sec:concepts:import}
% Describe how to use it, i.e. generic parameter and importer objects, and the fact that you can select between import and no import.

The import concept is a central part of the library.
It was developed to let users choose between two object passing strategies, namely the Data Processor and the Import approach (see Section \ref{sec:object-migration}).

The main class is the deferred \lstinline!CP_IMPORT_STRATEGY [G]!, which has the simple interface:

\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={The deferred class CP\_IMPORT\_STRATEGY.}]
deferred class interface
  CP_IMPORT_STRATEGY [G]

feature -- Status report

  is_importable (object: separate G): BOOLEAN
      -- Is `object' importable?

feature -- Duplication

  import (object: separate G): separate G
      -- Import `object'.
    require
      importable: is_importable (object)

end
\end{lstlisting}

%\lstinputlisting [firstline=7] {../../library/import/cp_import_strategy.e}

The class has two descendants: \lstinline!CP_NO_IMPORTER[G]! can be used for the Data Processor strategy. 
It just perform a reference copy of the object.
The class \lstinline!CP_IMPORTER [G]! on the other hand narrows the return type of \lstinline!import! to a non-separate \lstinline!G!, meaning that it actually performs an import.

As there's no general-purpose import feature available at the moment, a user has to implement his own feature for every class that needs to be imported.
Descendants of \lstinline!CP_IMPORTER! simplify this task and provide predefined implementations for some standard classes such as \lstinline!STRING!.
Those mechanisms are described in detail in Section \ref{sec:modules:import}.

Components that want to make use of the import mechanism need to have an instance of \lstinline!CP_IMPORT_STRATEGY! on the same processor.
There are several ways how this object can be supplied to a predefined component.
The most obvious solution - passing it as an argument in a constructor - has a big drawback in the SCOOP setting:
A user can't create the component on a separate processor without having to write an extra factory class.

A better solution is to exploit the constrained genericity mechanism in Eiffel.
Any component that wishes to import objects has to declare an additional generic argument for the import strategy.
A user can then decide on the precise semantics of the import strategy by just declaring the right type.

The constraint placed on the argument is that it needs to be a descendant of \lstinline!CP_IMPORT_STRATEGY!, and that it needs to declare \lstinline!default_create! as a creation procedure.
The latter is not a big restriction in practice, as there are usually no attributes in an importer anyway.

A typical class header of a component using the import concept looks like this:
\begin{lstlisting}[language=OOSC2Eiffel, captionpos=b, caption={An example component with import.}]
class BUFFER [G, IMPORTER -> 
	CP_IMPORT_STRATEGY [G] create default_create end]

feature -- Access

  item: like {IMPORTER}.import
end
\end{lstlisting}

This small code sample shows another neat little feature of Eiffel.
The \lstinline!like! statement fixes the type based on the chosen import strategy, i.e. \lstinline!separate G! for \lstinline!CP_NO_IMPORTER! and non-separate for descendants of \lstinline!CP_IMPORTER!.
With this trick a component can simplify the handling of imported references.

\subsubsection{Separate Proxy}

The separate proxy pattern simplifies access to a separate reference by providing a processor-local proxy object wich forwards all requests to the actual object.
The main advantage is that clients don't need to write extra ``wrapper function'' to control the separate object.
It is applied to all classes in the library which are meant to be shared among processors, i.e. which are usually accessed through a separate reference.

The pattern consists of three classes:
\begin{description}
 \item [Protégé] The actual business class whose objects are usually separate.
 \item [Helper] A class that provides helper functions to access a separate protégé.
  The helper class is usually ending on \lstinline!_UTILS!.
 \item [Proxy] A proxy class with the a similar interface as the protégé class, usually ending on \lstinline!_PROXY!.
    The proxy forwards all calls to its protégé, using the helper class.
\end{description}

It is possible to add a fourth, deferred class that just defines the interface for the protégé and the proxy.
However, there's an inconsistency: 
All preconditions in the protégé class that reference \lstinline!Current! (explicitly or implicitly) need to be weakened (i.e. \lstinline!require else True!) in the proxy, and turned into wait conditions in the helper class.
Furthermore, not all features in the business class may be necessary in the proxy, and the proxy itself may add some more features such as compound actions.

\begin{figure}[h]
\label{fig:separate-proxy}
\includegraphics[width=\textwidth]{separate_proxy.png}
\caption{The class relations in the Separate Proxy pattern.}
\end{figure}

Unfortunately this pattern cannot be fully turned into a reusable module, because it's highly dependent on the precise interface of the protégé class.
There is some support in the library however: 
\lstinline!CP_PROXY! defines the creation procedure and the two attributes \lstinline!subject! for a separate protégé object and \lstinline!utils! for a helper object.

Appendix \ref{sec:howto-separate-proxy} provides a general recipe on how to implement a separate proxy for an arbitrary protégé class.

\subsection {Module overview}
\label{sec:module-overview}
% Describe available modules, which patterns they're implementing, and which modules they depend upon.

The library consists of several modules which implement some of the patterns described in the overview (Section \ref{sec:pattern_overview}).

One of the most basic modules is the Import module in \dir{library/import}.
It implements the Import \patternref{IMP} pattern  and is at the same time one of the core concepts of the library.

The queue module in \dir{library/queue} implements the Prdocuer / Consumer \patternref{P/C} pattern.
It depends upon the import module.

The process module in \dir{library/process} provides skeleton classes for objects with a main loop.
It provides implementations for the Active Object \patternref{AO}, Asynchronous Self-Call \patternref{ASC} and Timer: Periodic \patternref{TP} patterns.

The worker pool module in \dir{library/worker\_pool} implements the Worker Pool \patternref{WP} pattern.
It depends on the import, queue and process module.

The future module is located in \dir{library/executor} and \dir{library/promise}.
It provides an implementation for the Future \patternref{FUT} as well as the Executor framework \patternref{EF}.

The class \lstinline!CP_DELAYED_TASK! in \dir{libary/util} implements the Timer: Invoke Later pattern \patternref{TIL}.

\input{modules}

\section{Evaluation}
\label {sec:evaluation}

To evaluate the library we implemented a small performance benchmark.
We implemented the Gaussian elimination algorithm in three different ways: sequentially, with SCOOP only, and using the future module (see Section \ref{sec:futures} from the library.
We chose the future pattern because it indirectly also measures many other parts of the library, like the worker pool or import mechanism.

We ran the tests with randomly generated matrices and each test was repeated 5 times.
A test matrix was square and its order always a power of two in the range from 32 to 1024.
Additionally, there was one more column for the result vector in the system of linear equations.
The test system was a quad-core AMD Phenom II X4 955 processor with 6 GB of RAM.
The results of the tests are shown Table \ref{table:perf-results}.

\begin{table} [!h]
\centering
\begin{tabular}{|l|l l l|} 
\hline
Matrix Size & Future & Raw SCOOP & Sequential\\
\hline
32 & 0.36 & 0.19 & 0\\
64 & 1.67 & 1.11 & 0\\
128 & 8.45 & 9.27 & 0.04\\
256 & 26.45 & 66.56 & 0.33\\
512 & 102.28 & 515.11 & 2.62\\
1000 & - &  3937.2 & - \\
1024 & 424.32 & error & 20.79 \\
\hline
\end{tabular}
\caption{Average time for different algorithms.}
\label{table:perf-results}
\end{table}

\todo{Maybe add small data plot.}

From the results we can get several observations:

\begin{itemize}
 \item The raw SCOOP solution fails for the biggest matrix. 
 This is because it uses more than the maximum number of processors, and it's a known bug \cite{web:scoop-issues}.
 The solution with futures doesn't suffer from this problem because it's using a fixed amount of processors.
 \item Futures are a lot faster than the raw SCOOP solution on large data sets.
 \item For smaller data sets, raw SCOOP beats the library.
 \item Sequential execution is much faster than SCOOP.
\end{itemize}

The last observation is probably the most fundamental.
The SCOOP runtime really needs to be improved in order to make it competitive to threaded systems, or even sequential ones.
Fortunately an improved version \cite{thesis:scottwest} is being developed at the time of writing.
It should be integrated into a future EiffelStudio release.

Another improvement which might be useful for the library is the Passive Processor concept \cite{paper:passive-processors}.
Both the worker pool and the promise object processors in the future module could be declared passive, which might speed up the computation a lot.

\section{Conclusion}
% Explain what is the impact of the selected patterns, and why they were selected.
% Say that import was developed to overcome language limitation
% Also: Future work

%Applications using multiple processor cores are increasingly becoming the norm, despite the difficulties of dealing with parallelism.
In this thesis we've worked out many methods that simplify concurrent programming in one way or another.
We've done a survey of concurrency patterns and described them in an extensive list.
This list can be used by anyone searching for a particular pattern.

From this list we selected some patterns which we thought to be especially useful.
The selection was based on the study of other concurrency libraries as well as some input from the Software Engineering research group at ETH.

The seleted patterns were then implemented and are now available as a new Eiffel library.
Besides the actual pattern implementations, this library also provides some workarounds for current SCOOP limitations, such as the missing import statement.

Performance measurements for the Future pattern implementation showed that the library is actually faster for large data sets and uses less threads than the native SCOOP approach.

Finally we also describe some challenges when programming in SCOOP, and how they can be solved.
This is especially useful to programmers new to SCOOP, but which may have experience in concurrent programming with threads.

\subsection{Future work}

The library provides several opportunities for future work.

\begin{description}
 \item [More patterns] The library can be extended with further patterns.
 It may be useful to include Pipeline or Dataflow network.
 \item [Separate Proxies] It may be useful to apply the Separate Proxy patterns to some EiffelBase classes, such as \lstinline!ARRAYED_LIST!, \lstinline!HASH_TABLE! or \lstinline!ROUTINE!.
 \item [Separate Proxy Wizard] The creation of a separate proxy can be mostly automated with a wizard.
% \item [EiffelVision support] \todo{maybe?}
% \item [Agent integration] \todo{maybe?}
 \item [Concurrent Datastructures] Sometimes it may be useful to have truly concurrent data structures for performance reasons.
The Array Slicing technique \cite{paper:array-slicing} is an example how arrays with concurrent read access can be implemented in SCOOP.
\end{description}

A important step is to improve SCOOP itself.
There are various ways to do it, most of them related to performance improvements.

\begin{description}
 \item [Faster runtime] The SCOOP runtime needs to become faster. 
 This is currently being developed \cite{thesis:scottwest}.
 \item [Native Import] A native SCOOP import feature is a good tool to deal with a lot of small objects.
 It will also make the library code and usage simpler, as the manual import workaround can be removed.
 \item [Passive Processors] The passive processors concept \cite{paper:passive-processors} could be integrated into EiffelStudio.
 It can make a big performance improvement to situations where one needs to pass data from one processor to another.
 \item [Separate references] The handling of separate references should become more simple.
 At the moment a programmer is forced to write a lot of small, annoying features to perform separate calls.
 Some syntactic sugar would really be helpful.
\end{description}


\begin{flushleft}
{{{
\bibliographystyle {plain}
\bibliography {./references}
}}}
\end{flushleft}


\begin{appendices}
% \input{tutorial}
\input{separate_proxy}
\section{SCOOP and EiffelVision}
\todo{An appendix for EiffelVision support.}
\end{appendices}

\todos

\end{document}          
