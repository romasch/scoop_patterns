\section{Library modules}
\label{sec:modules}

\subsection{Import}
\label{sec:modules:import}

The import module substitutes the SCOOP \lstinline!import! feature, a built-in mechanism that is unfortunately not implemented at the moment.
% It consists of all classes in the directory \dir{library/import}. \\
The basic concepts and ideas behind the module are described in Section \ref{sec:concepts:import}.
This section only deals with the class \lstinline!CP_IMPORTER! and its descendants.

The class \lstinline!CP_IMPORTER [G]! has a single deferred feature \lstinline!import!.
It does not provide a generic import mechanism.
To write an importer for an arbitrary type, e.g. \lstinline!STRING!, a client needs to write a new class, inheriting from \lstinline!CP_IMPORTER [STRING]!, and implement the deferred feature.

Although the library has some predefined importers, e.g. for \lstinline!STRING!, writing an extra class for every user-defined type may quickly become tedious.
Therefore the library provides another way of using the import module with the class \lstinline!CP_IMPORTABLE!:

\begin{lstlisting}[captionpos=b, caption={The deferred class CP\_IMPORTABLE.}]
deferred class
  CP_IMPORTABLE

feature {CP_DYNAMIC_TYPE_IMPORTER} -- Initialization

  make_from_separate (other: separate like Current)
      -- Initialize `Current' with values from `other'.
    deferred
    end

end
\end{lstlisting}

Users can inherit from \lstinline!CP_IMPORTABLE! and define the import function right inside their class.

There are two predefined importers which can be used for \lstinline!CP_IMPORTABLE! objects: 
\begin{itemize}
 \item \lstinline!CP_DYNAMIC_TYPE_IMPORTER!
 \item \lstinline!CP_STATIC_TYPE_IMPORTER!
\end{itemize}
The latter uses constrained genericity to create an object of type \lstinline!G!.
The approach is pretty simple and fast but it has the drawback that the result type is always the static type \lstinline!G!, even if the argument to \lstinline!import! was of a subtype of \lstinline!G!.

The \lstinline!CP_DYNAMIC_TYPE_IMPORTER! on the other hand respects the dynamic type of its argument.
With the help of reflection it creates a new, uninitialized object of the correct type and then calls \lstinline!make_from_separate! to perform the initialization.
This introduces a new problem however with respect to void safety.

As opposed to the static type importer, the feature \lstinline!make_from_separate! doesn't need to be a creation procedure.
This in turn means that the compiler will not check if every attribute is correctly initialized.
% The object may not be correctly initialized if \lstinline!make_from_separate! is not declared as a creation procedure, as the new object will not be initialized.
It is therefore strongly advised to declare \lstinline!make_from_separate! as a creation procedure for every descendant of \lstinline!CP_IMPORTABLE!.

Another problem of the \lstinline!CP_DYNAMIC_TYPE_IMPORTER! is the invariant of an object.
There's a short time interval between the creation of an object (using reflection) and the call to \lstinline!{CP_IMPORTABLE}.make_from_separate! where the invariant is broken.
Due to this it is impossible to use classes with invariants in conjunction with the dynamic type importer.

In the future, there will hopefully exist an import routine natively supported by the SCOOP runtime.
In that case \lstinline!CP_IMPORTER! can be made effective and use the native import, and all its descendants will become obsolete.

\subsection{Queue}

The queue module provides the class \lstinline!CP_QUEUE! and some support classes that implement the Separate Proxy pattern \patternref{SP}.
The module can be used for the Producer / Consumer pattern \patternref{P/C}.
% It is mostly used to easily implement the producer / consumer pattern in a concurrent context.

The main challenge in the queue module is data migration, as described in Section \ref{sec:object-migration}.
Therefore the module makes heavy use of the import concept.
This means that, along with a generic argument for the data type, it is also necessary for clients to provide a \lstinline!CP_IMPORT_STRATEGY!.
The import strategy basically ``teaches'' the queue how to import a given object.

Internally, \lstinline!CP_QUEUE! uses an \lstinline!ARRAYED_LIST! to store its elements.

% It uses the separate proxy pattern, which means that it consists of three classes:
% 
% \begin{itemize}
%  \item \lstinline!CP_QUEUE!
%  \item \lstinline!CP_QUEUE_UTILS!
%  \item \lstinline!CP_QUEUE_PROXY!
% \end{itemize}
% 
% The first one provides the actual queue, but internally it just relies on \lstinline!ARRAYED_QUEUE!.
% The class \lstinline!CP_QUEUE_PROXY! can be used to access such a shared, separate queue without having to deal with separate references.
% 
% The interesting thing about this queue implementation is that it makes use of the import module.
% Along with the generic argument \lstinline!G! you can also provide an \lstinline!CP_IMPORT_STRATEGY [G]!.
% The import then happens automatically in both the queue and its proxy.

% \subsubsection{Producer / Consumer}
% 
% 
% The producer / consumer is a very popular pattern in concurrent programming, and it is a building block for other patterns like pipeline as well.
% The basic idea is to have a shared, concurrent buffer.
% Producer threads put new items into the buffer, whereas consumer threads remove items from this buffer.
% 
% \todo{BON-style diagram and ``migration graphics''}
% 
% In threaded systems, this buffer is usually accessed by several threads at the same time.
% Careful synchronization has to be enforced to ensure that the buffer remains in a consistent state.
% The data items on the other hand ``migrate'' from a producer to the buffer, and then to exactly one consumer.
% They therefore don't need to be thread-safe as long as the producer promises never to touch the item again.
% 
% In SCOOP things look a bit different however.
% Due to the exclusive access guarantee it is not necessary to establish a synchronization policy.
% The downside however is a loss of potential concurrency when a producer and a consumer accesses the buffer simultaneously, but this is a minor problem.
% 
% The main problem in SCOOP are the data items, especially if they are not of an expanded type.
% If the object is created on the producer processor, then the consumer needs to control the producer in order to get access to the object.
% This is clearly a situation that we want to avoid, because it couples the producer and consumer in a vicious hidden way, and the whole point of the producer / consumer pattern is to decouple the two.
% 
% A nice solution would be if it's somehow possible to migrate data items, like it's done in threaded languages.
% However, this is not possible with the current semantics of SCOOP, because an object always stays on the processor it was created on.
% 
% One approach to solve this problem is to create a new processor for every data item.
% This actually works, but it can be very slow, especially for a lot of small data items.
% There are two reasons for this:
% First, every SCOOP processor is mapped to an operating system thread, therefore creating a new processor involves creating a new thread which is an expensive operation.
% The second reason is the overhead of separate calls itself.
% This has to be paid every time the consumer wants to access the separate object.
% 
% Another problem of this approach is related to ease of programming.
% Dealing with a separate object can be very annoying, because you need to write small wrapper functions for every little feature call.
% 
% \begin{lstlisting}
% class
%   CONSUMER
% 
% feature -- Basic operations
%   
%   retrieve
%       -- Retrieve a string and print its length.
%     local
%       l_string: separate STRING
%     do
%       l_string := buffer_consume (a_queue)
%       print (string_count (l_string))
%     end
%     
% feature {NONE} -- Implementation
%   
%   buffer: separate BUFFER [STRING]
% 
%   buffer_consume (a_buffer: like buffer): separate STRING
%       -- An annoying small wrapper function for a buffer.
%     do
%       Result := a_buffer.item
%       a_buffer.remove
%     end
%     
%   string_count (a_string: separate STRING): INTEGER
%       -- An annoying small wrapper function for a string.
%     do
%       Result := a_string.count
%     end
% end
% \end{lstlisting}
% 
% 
% Due to these problems we decided to go for a different strategy: cloning objects.
% Using the import module it is possible to ``teach'' the shared buffer how to clone any user-defined object by just providing a generic argument.
% A first library for the producer / consumer pattern thus consisted of the class \lstinline!CP_QUEUE! and \lstinline!CP_IMPORT_STRATEGY!, along with some predefined importers.
% 
% The import trick solves the main problem of the producer / consumer, namely migrating objects from producer to consumer efficiently.
% However, producers and consumers still have to deal with a nasty separate reference (the shared buffer), and there's also the problem that a user of the library might forget to import objects on the consumer side.
% 
% To overcome this problem we implemented a non-separate proxy class which automatically deals with the separate reference and imports.
% This idea proved to be so successful that eventually it was turned into its own pattern: the separate proxy.
% 
% \todo {Bon-style graphics of CP\_QUEUE and related items.}


\subsection{Process}

The process module provides a set of classes that all implement a skeleton for a main loop with a deferred body.

The class \lstinline!CP_PROCESS! defines the interface.
It is a descendant of the class \lstinline!CP_STARTABLE!, which means that clients have a simple way to start a separate process using \lstinline!CP_STARTABLE_UTILS!.

Users need to implement the feature \lstinline!step!, which should contain the body of the loop.
The feature \lstinline!start! is used to start the loop, and it can be terminated by setting the attribute \lstinline!is_stopped! to \lstinline!True!.
% As the process module defines the skeleton for a main loop, users are just required to implement \lstinline!step!, which should contain the body of the loop.

\lstinline!CP_PROCESS! also introduces the two methods \lstinline!setup! and \lstinline!cleanup!.
They are called in the beginning or at the end of the main loop, and must be explicitly redefined by descendants if needed.

There are two techniques to implement the main loop itself.
The first technique, used by \lstinline!CP_CONTINUOUS_PROCESS!, is pretty staightforward:
\begin{lstlisting}
from setup
until is_stopped
loop
  step
end
\end{lstlisting}
This approach is very simple and fast. 
The problem however is that other processors never get a chance to access the \lstinline!CP_CONTINUOUS_PROCESS! unless the main loop is exited completely.
This class is a simple implementation of the Active Object pattern \patternref{AO}.

\lstinline!CP_INTERMITTENT_PROCESS! implements the other technique which is more interesting.
The basic idea is to perform only one iteration, and then ask some other processor to invoke the loop body again in \lstinline!Current!.
This ping-pong approach ensures that other processors get a chance to access and modify data in \lstinline!CP_INTERMITTENT_PROCESS! after each iteration.
In practice this is particularly useful to stop a \lstinline!CP_INTERMITTENT_PROCESS! from the outside.

\lstinline!CP_INTERMITTENT_PROCESS! implements the Asynchronous Self-Call pattern \patternref{ASC}.
The callback service is provided by the class \lstinline!CP_PACEMAKER!.
Every \lstinline!CP_INTERMITTENT_PROCESS! automatically creates an associated pacemaker.

The \lstinline!CP_PERIODIC_PROCESS! allows to add small delays between executions. 
It is a descendant of \lstinline!CP_INTERMITTENT_PROCESS! and an implementation of the Timer: Periodic pattern \patternref{TP}.
The class also introduces the simple command \lstinline!stop!, which can be used to stop the timer.


\subsection{Worker pool}
\label{sec:worker_pool} 

A worker pool is a set of threads that are ready to execute tasks.
The intention of the worker pool is to make use of parallelism while avoiding the overhead of expensive thread creation.

The main component of the worker pool is a shared buffer where clients can insert tasks to be executed.
A set of worker threads then continuously retrieve tasks from the buffer and execute them.
The worker pool module makes use of the queue module which provides the shared buffer.

The representation of a task to be executed varies between different languages.
In Java for instance a Runnable object is used, whereas in C\# the task is represented as a delegate.
SCOOP however has to deal with the problem of object migration, as described in Section \ref{sec:object-migration}.

If the task object is created on its own processor, as in the Data Processor approach, the performance advantage of the worker pool cancels out.
With the Lock Passing approach a task object will be executed on the processor that created the object, which kind of makes the worker pool useless (not to mention the risks of starvation if applied wrong).
This only leaves the import mechanism as a sensible solution.

The library supports two flavors of a worker pool.
The first and more basic one is to have a deferred class \lstinline!CP_WORKER! where clients can directly implement an operation.
The object submitted to the worker pool then corresponds to the arguments of the operation.

The second solution uses a special class to encapsulate an operation.
It is described in Section \ref{sec:executor}.

The basic worker pool module has three important classes:
\begin{itemize}
 \item \lstinline!CP_WORKER_POOL!
 \item \lstinline!CP_WORKER!
 \item \lstinline!CP_WORKER_FACTORY!
\end{itemize}

The \lstinline!CP_WORKER_POOL! provides the shared buffer and some additional functionality to adjust the pool size.
The type of the task object alongside its import strategy can be specified with generic arguments.
\lstinline!CP_WORKER_POOL! inherits from \lstinline!CP_QUEUE! and therefore uses the same import concept.

The deferred class \lstinline!CP_WORKER! corresponds to the worker thread in other languages.
Users need to implement the feature \lstinline!do_run!, which receives a task object and executes some operation on it.
The exact type of the task object depends on the generic arguments of \lstinline!CP_WORKER!, which must be the same as in \lstinline!CP_WORKER_POOL!.
The non-deferred part of \lstinline!CP_WORKER! is the main loop itself, which fetches a new task, calls \lstinline!do_run!, and checks if the worker needs to terminate.

The last class, \lstinline!CP_WORKER_FACTORY!, just provides a deferred factory function for a new worker.
The factory class is necessary because the exact type of \lstinline!CP_WORKER! is not known to the library in advance.
\lstinline!CP_WORKER_POOL! uses the factory to create new workers on demand.

% \subsubsection{Terminating workers}

An important functionality of a worker pool is to adjust the number of workers.
Increasing the worker count is easily done by just creating new instances of \lstinline!CP_WORKER!.
To decrease the amount of workers however the module needs to apply the processor termination technique described in Section \ref{sec:processor-termination}.


% However, decreasing the amount of workers is not that easy.
% 
% Java provides two builtin mechanisms to shut down a thread.
% You can either force it to stop, which immediately throws an exception in the thread \todo{ref}, or you can use the more collaborative interrupt mechanism \todo{ref}.
% The idea is that the thread will regularly check its interrupted flag and terminate on its own if requested.
% 
% The latter is also possible to do in SCOOP, except that there's no builtin interrupt flag.
% Instead a query \lstinline!is_stop_requested! in \lstinline!CP_WORKER_POOL! can be used.
% The main problem however are wait conditions.
% 
% In Java, blocking calls like \lstinline!wait()! and \lstinline!sleep()! may throw an \lstinline!InterruptedException! \todo{ref}.
% This avoids the problem that a thread may wait forever instead of shutting down, because all signaller threads have already terminated.
% Unfortunately, there's no such mechanism in SCOOP.
% It is possible however to work around this limitation by refining the wait condition:
% \begin{lstlisting}
% 
% class
%   CP_WORKER
%   
%   -- ...
%   
% feature -- Implementation
% 
%   fetch (pool: separate CP_WORKER_POOL)
%     require
%       not pool.is_empty or pool.is_stop_requested
%     do
%       if is_stop_requested then
% 	-- Stop the currrent worker.
%       else
% 	-- Grab the next item.
%       end
%     end
% 
% end
% \end{lstlisting}
% The additional \lstinline!if! statement is not very nice, but luckily it can be encapsulated completely in \lstinline!CP_WORKER!.
% 
% This code snippet is useful to break free of any wait condition if the requirements have changed.


The Separate Proxy pattern \patternref{SP} is applied to \lstinline!CP_WORKER_POOL! to make the handling of a separate worker pool object more convenient.

The basic worker pool module is very flexible.
It is for example possible to use it just as an advanced producer / consumer module where consumers are automatically created and destroyed.
The drawback however is that clients need to implement two classes, the worker and the factory, to make use of the module.
Section \ref{sec:executor} therefore introduces a more specialized version of the worker pool which can be used to execute arbitrary operations.



% \subsubsection{Arbitrary operations}
% \label{sec:arbitrary-operations}
% 
% So far the task of a worker is defined in a user-defined \lstinline!CP_WORKER! class, and the object submitted to the worker pool mostly contains data.
% The worker pool implementations in Java and C\# only accept Runnable (or delegate) objects.
% This enables arbitrary operations that can be executed by the worker threads.
% 
% The SCOOP version of the worker pool can be enhanced to act like the Java / C\# counterparts.
% To do that we need a class that represents an operation, and which can be moved across processor boundaries.
% 
% The agent classes in Eiffel (i.e. ROUTINE and descendants) may be used to represent operations, but they can't be easily imported.
% That's why we added a new, deferred class \lstinline!CP_TASK!.
% Users of the library can inherit from it and implement the feature \lstinline!run!.
% \todo {Tell about agent integration?}
% 
% Using this interface it is possible to have a predefined \lstinline!CP_TASK_WORKER! that just runs \lstinline!CP_TASK! objects.
% The associated \lstinline!CP_TASK_WORKER_POOL! implements the factory function and refines the raw \lstinline!CP_WORKER_POOL!.
% 
% The combination of these two classes is very close to the Java worker pool implementation.
% The only difference is that a \lstinline!CP_TASK! object needs to be imported, whereas a Java Runnable object doesn't.
% 
% \todo{Maybe merge subsections Arbitrary Operations and Promise into a new section that describes the ideas behind CP\_TASK?}

\subsection{Promise}
\label {sec:promise}

The promise module contains a set of classes which can be used to monitor the state of an asynchronous operation.
It is mostly used in conjunction with the executor or future module.

The main class is \lstinline!CP_PROMISE!, which defines queries like \lstinline!is_terminated! or \lstinline!is_exceptional!.
It also defines the interface to cancel an operation or to get the progress percentage (e.g. for a download task), but these queries need to be supported by the asynchronous operation as well.

The Separate Proxy \patternref{SP} is available for promise objects, because they are usually declared separate to the client.
In this case the pattern is implemented with four classes, i.e.
\begin{itemize}
 \item \lstinline!CP_PROMISE! defines a common interface,
 \item \lstinline!CP_SHARED_PROMISE! defines the actual separate object,
 \item \lstinline!CP_PROMISE_UTILS! has features to access a \lstinline!separate! \lstinline!CP_PROMISE! and
 \item \lstinline!CP_PROMISE_PROXY! implements the proxy object.
\end{itemize}

There's an important descendant, the \lstinline!CP_RESULT_PROMISE!, which is used for asynchronous operations returning a result.
It also has a set of associated classes that implement the Separate Proxy pattern.

The \lstinline!CP_RESULT_PROMISE! contains a query \lstinline!item! to retrieve the result as soon as it is available.
A distinguishing feature of this query is that it blocks if the result is not yet available.

The return type of \lstinline!item! depends on a generic argument.
To move the result back to the client the class makes use of the import concept.
This means that both \lstinline!CP_SHARED_RESULT_PROMISE! and \lstinline!CP_RESULT_PROMISE_PROXY! have an additional generic argument which defines the import strategy.

\subsection{Executor}
\label{sec:executor}

The executor module defines an interface for executing arbitrary tasks.
The implementation of the execution service may vary.
In most cases it is a worker pool, but it is also possible to use a single thread or to execute the task synchronously in the current thread.

The representation of a task object in Java is a Runnable object, or a delegate in C\#.
A SCOOP implementation also needs a class to represent an task, but with an important additional requirement: 
Its objects have to be importable.

The agent classes in Eiffel (i.e. \lstinline!ROUTINE! and descendants) may be used to represent operations, but they can't be easily imported.
Therefore we added a new, deferred class \lstinline!CP_TASK! to represent an importable asynchronous operation.
It also adds some additional functionality like exception handling or the ability to add a promise object (see Section \ref{sec:promise}).
To define a new task clients need to inherit from \lstinline!CP_DEFAULT_TASK! and implement the two features \lstinline!run! and \lstinline!make_from_separate!.

The interface to execute tasks is provided by the class \lstinline!CP_EXECUTOR!.
It defines the feature \lstinline!put! which takes a \lstinline!separate CP_TASK! object as its argument.
As an executor instance is usually placed on its own separate processor we applied the Separate Proxy pattern \patternref{SP} on \lstinline!CP_EXECUTOR!.

\todo {Tell about agent integration?}

The executor framework is pretty useless on its own, as it essentially consists of only two deferred classes.
Therefore it is shipped with a worker pool implementation.
The \lstinline!CP_TASK_WORKER_POOL! implements the executor interface and is itself a descendant of the more basic \lstinline!CP_WORKER_POOL!.
The associated \lstinline!CP_TASK_WORKER! then just fetches \lstinline!CP_TASK! objects and executes them.


\subsection{Futures}
\label{sec:futures}

The Future pattern \patternref{FUT} is used to perform a computation asynchronously.
Instead of computing a value straight away, the computation is wrapped into an object and the user receives a handle to retrieve the value as soon as it is ready.
This handle is often called Future, Promise or Delay.
In this section we'll use the term future for the whole pattern, and promise only refers to the handle.

The main advantage of the future pattern is that it allows to make use of parallelism in an easy way.
Users just have to spot computations which may run asynchronously and the future pattern then takes care of thread management, synchronization and result propagation.

The pattern consists of four building blocks:
\begin{itemize}
 \item The promise,
 \item the computation,
 \item the execution service,
 \item and a ``frontend'' object which takes a computation, submits it to the executor, and returns a promise.
\end{itemize}

The representation of the computation is a Callable object in Java and a delegate in C\#.
Our library uses the class \lstinline!CP_COMPUTATION! with the deferred feature \lstinline!computed!.
It is a descendant of \lstinline!CP_TASK! introduced in Section \ref{sec:executor}.

The promise object is defined by \lstinline!CP_PROMISE! and its descendants.
The detailed implementation is described in Section \ref{sec:promise}.

Due to the fact that \lstinline!CP_COMPUTATION! inherits from \lstinline!CP_TASK! we can just use the executor module (see Section \ref{sec:executor}) as the execution service for the future pattern.

The ``frontend'' part is provided by the two classes \lstinline!CP_EXECUTOR_PROXY! and \lstinline!CP_FUTURE_EXECUTOR_PROXY!.
This is an example for a proxy object where the responsability has been expanded:
Instead of just forwarding the computation object to the execution service, it also initializes the promise and returns it to the user.

The implementation of the future pattern hits two challenges:
\begin{itemize}
 \item Object Migration (see Section \ref{sec:object-migration}): Operations can't be easily moved from the client to an execution service.
 The same is also true for the result of a computation in the reverse direction.
 \item Processor Communication (see Section \ref{sec:processor-communication}): The promise object should neither be placed on the client processor nor on the executor service.
% The reason in both cases is that one processor may execute a main loop, which means the other processor never gets access to the promise object.
\end{itemize}

The first problem is already solved by the executor module. 
Just like a \lstinline!CP_TASK! object, a \lstinline!CP_COMPUTATION! is movable across processor boundaries.
To bring the result back to the client the promise module makes use of the import concept.

% (Section \ref{sec:concepts:import}.
% This means that both the Executor service and the Promise object have a generic argument to define the \lstinline!CP_IMPORT_STRATEGY!.
% The executor service always uses \lstinline!CP_DYNAMIC_TYPE_IMPORTER!, because \lstinline!CP_COMPUTATION! inherits from \lstinline!CP_IMPORTABLE!.
% The import strategy of the Promise object however is user-defined.
The second problem is more interesting however.
As we've seen in Section \ref{sec:processor-communication}, the promise object needs to be placed on a third processor.
However, starting a new processor for every computation introduces a huge overhead.

A better tradeoff would be to create one global processor which takes care of all promise objects.
This may introduce some contention if multiple futures are submitted at the same time, but we think that this is acceptable.

The global processor approach brings another problem though.
A promise object has two generic arguments for the return type and the import strategy.
As these arguments are not known in advance, and because SCOOP processor tags \cite[p. 90]{Nienaltowski07} are not implemented yet, it is impossible to create a promise object on this dedicated processor.

The solution is - surprisingly - the import concept.
We can create a ``template'' promise object with the correct types on the client processor, and then ask the global processor to import it.
That way the promise ends up on the correct processor.